Mechanistic models have long been the gold standard of theoretical modeling in ecology (e.g. see @Geritz2012 or @Cuddington2013).  Only by understanding the processes involved can we make reliable long term predictions and build an knowledge of cause and effect that guides the hypotheses we make, the data we collect, and the management decisions we make.  Process-based models, whether expressed in the language of mathematics or English, identify the connection between mosquitoes and the spread of malaria, or greenhouses gases and climate change, guiding our approach to understand and manage these threats. Despite this central importance, I argue here that ecologists would do well to give greater attention to the role non-mechanistic models can play in ecological management and decision making.  The value of these approaches is greatest in a context where decisions are made over short time horizons, and updated as new data becomes available. 

A simple example will illustrate many of these points.  

Mechanistic modeling emphasizes the importance of capturing the correct gross properties of a system over the tracking minute fluctuations.  For instance, in selecting and parameterizing model of a population of conservation concern, we may be most interested in getting the long-term behavior correct -- such as identifying if the dynamics support persistence of the population -- rather than worrying how well they reflect the year-to-year fluctuations.  We would certainly have good reason to prefer such a model over alternatives which are irreconcilable to the most basic biological processes, such as unbounded growth, or growth curves that do not pass through the origin in a closed system. So it may come as a surprise to realize that such obviously wrong models can perform as well or even better than reasonable mechanistic models in guiding ecological management and decision making.  

An example of such a comparison is shown in Figure 1, in which the optimal management decision is determined by stochastic dynamic programming algorithm using the biologically (a) plausible and (b) the implausible model of the population growth.  

Three elements 

1. The dynamics occur over a range of state-space in which the biologically implausible model performs as well or better than the more plausible model.

  * 

2. This application relies only on the predictive accuracy of the model, not an intepretation of the parameters. 

  * Mechanistic modeling is at its most powerful not when it is used to directly forecast future states but when it provides an understanding of how to approach a problem.  Simple epidemiological models 

3. We are presented with new data after each subsequent decision.  

  * The relevant timescale for the prediction is thus not the long-term dynamics, which would be wildly divergent, but the dynamics over the much shorter interval between subsequent decisions.  



on the 

The danger of mechanistic models



## Better the devil you know?  The danger of phenomenological approaches

Few examples illustrate the danger of phenomenological modeling more broadly than the sub-prime mortgage crisis of 2008.  Simplifying greatly, the unquenchable demand for mortgage backed securities had shifted the underlying dynamics on which the phenomenological models were based faster than the models could learn about the shift.  Historical data indicated that mortgage-backed securities involved little risk.  






Parametric models are frequently used phenomenologically, rather than being derived by plausible underlying mechanisms.  @Geritz2012 condemn this practice on the basis of the common misinterpretations that result.  These mistakes arise from ignoring or misinterpreting the biological meaning of the model parameters. The use of delay equations is a frequent offender -- for instance, one is hard-pressed to describe a mechanistic model consistent with Hutchinson's 1948 delay-logistic.  Assuming either scramble competition or interference competition gives rise to two alternative delay equations, both structurally different from the original.  

Three elements contribute to this.  

phenomenological accuracy over 


$$ $$


A process-based understanding helps us formulate hypotheses, guide experimental design and data collection, predict outcomes, and influence management decisions.  The alternative to a processed-based model is  a correlative one. Statical models machine learning.  




> since the late 1980s there has been some consensus amongst ecologists that management decisions are best guided by models which are grounded in ecological theory, and which strike a balance between too much or too little detail describing the relevant processes (e.g., DeAngelis 1988, Starfield 1997, Jackson et al. 2000, Carpenter 2003, Nelson et al. 2008) 

machine learning algorithms 

Prediction without 

Example of Statistical model vs process-based model.  




@Geritz2012 @Cuddington2013 


Parametric doesn't mean process-based/mechanistic



Managing by rule-of-thumb

Managing by algorithm


