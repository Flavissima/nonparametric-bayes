\documentclass[author-year, review]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}
\usepackage{lineno} % add 
%\linenumbers % turns line numbering on 
\bibliographystyle{elsarticle-harv}
\biboptions{sort&compress} % For natbib
\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{\it \hfill\today}%
 \let\@evenfoot\@oddfoot}
\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header



\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{color}
\usepackage{fancyvrb}
\DefineShortVerb[commandchars=\\\{\}]{\|}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable}
\usepackage{graphicx}
% We will generate all images so they have a width \maxwidth. This means
% that they will get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
\else\Gin@nat@width\fi}
\makeatother
\let\Oldincludegraphics\includegraphics
\renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=\maxwidth]{#1}}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Non-parametric approaches to optimal policy are more robust},
            colorlinks=true,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header



\begin{document}
\begin{frontmatter}
  \title{Non-parametric approaches to optimal policy are more robust}
  \author[cstar]{Carl Boettiger\corref{cor1}}
  \author[cstar]{Marc Mangel}
  \author[noaa]{Stephan B. Munch}
  \ead{cboettig@ucsc.edu}
  \cortext[cor1]{Corresponding author, cboettig@ucsc.edu}
  \address[cstar]{Center for Stock Assessment Research, Department of Applied Math and Statistics, University of California, Mail Stop SOE-2, Santa Cruz, CA 95064, USA}
  \address[noaa]{Southwest Fisheries Science Center, National Oceanic and Atmospheric Administration, 110 Shaffer Road, Santa Cruz, CA 95060, USA}
 \end{frontmatter}


\section{Abstract}

\section{Introduction}

Most management recommendations from the ecological literature are based
on (or at least motivated by) parametric models. Though in principle
these models reflect mechanistic underpinnings of biological
interactions involved, in practice real mechanisms are often unknown and
the true dynamics too complex to be captured by simple models
{[}@refs{]}. While simple mechanistic models can nevertheless provide
imporant insights into possible dynamics -- for instance, demonstrating
that a vaccine does not have to be 100\% effective to eliminate the
transmission of a virus {[}@Kermack1921{]} -- such approaches are not
well-suited for use in forecasting future dynamics upon which outcomes
management policy can be based. Despite a long history of work
emphasizing the difference between modeling for understanding (generic
or strategic models) and modeling for prediction (precise, specific, or
tactical models) (e.g. Levins 1966), simple parametric models continue
to find application in predictive management contexts. The management of
marine fisheries is a prime example, in which simple, mechanistically
motivated models of stock-recruitment relationships such as Ricker or
Beverton-Holt curves are systematically fit to data and used in solving
decision-theoretic or optimal control problems determining resource
management policies such as maximum sustainable yield {[}@refs{]}.

Stochastic dynamic programming @Mangel1982

To address structural uncertainty in the processes involved, such
mechanistic models can be adapted to accomidate uncertainty in the
parameters or consider a set of alternative models simultaneously
(belief SDP)

Further unknowns such as measurement uncertainty, parameter uncertainty,
unobserved states, knowledge of boundary conditions, etc. further
compound the issue. Though a hierarchical Bayesian approach provides a
natural way to address these from a statistical standpoint, formulating
reasonable parametric descriptions of each form of uncertainty is a
challenging task in itself, let alone the computational difficulties of
solving such a system. Cressie et al. (2009) provides a good account of
the successess and challenges of the approach. Applying these approaches
in the management context of sequential decision making, in which
forecasts must be obtained over a range of possible actions and updated
regularly as new information arrives makes such an approach less
feasible still.

Though machine learning approaches have begun to appear in the
ecological and conservation literature (Species distribution models),
including the Gaussian process based approach used here (Munch et al.
2005), they remain unfamiliar and untrusted approaches for most
ecologists. Machine learning approaches represent an essentially
pattern-based rather than process-based approach, raising the same
skepticism from most theoretical ecologists that they hold for older
correlative methods such as linear regression, while the complexity of
the techniques has barred their adoption in more empirical audiences.

A more immediate barrier to their adoption is the absence of a framework
for applying machine learning approaches to resource management
problems. Traditional approaches to optimal control (Pontryagin's
principle, stochastic dynamic programming) rely on knowledge of the
state equation, usually described by a simple parametric model. Here we
illustrate how a stochastic dynamic programming algorithm can
alternatively be driven by the predictions from a Gaussian process -- a
machine learning approximation to the state dynamics.\\

Management goals / decision-theoretic approaches need accurate
prediction over relevant (short?) timescales more than reasonable
long-term mechanisms. Machine-learning approaches may offer the benefit
of the hierarchical Bayesian approach without the practical and
computational limitations of their parametric kin. Non-parametric models
are flexible enough to take maximum advantage of the data available,
while being appropriately ambiguous about the dynamics of a system in
regions of parameter space that have been poorly or never sampled.

\section{Approach and Methods}

\subsubsection{Background on Gaussian Process inference}

The use of Gaussian process regression (known as Kreging in the
geospatial literature) to formulate a predictive model is relatively new
in the context of modeling dynamical systems {[}@Kocijan2005{]} and
introduced in the ecological modeling and fisheries management by Munch
et al. (2005). An accessible and thorough introduction to the
formulation and use of Gaussian processes can be found in
@Rasmussen2006.

The essense of the Gaussian process approach can be captured in the
following thought experiment: An exhaustive parametric approach to the
challenge of structural uncertainty might proceed by writing down all
possible functional forms for the underlying dynamical system with all
possible parameter values for each form, and then consider searching
over this huge space to select the most likely model and parameters; or
using a Bayesian approach, assign priors to each of these possible
models and infer the posterior distribution of possible models. The
Gaussian process approach can be thought of as a computationally
efficient approximation to this approach. Gaussian processes represent a
large class of models that can be though of as capturing or reasonably
approximating the set of models in this collection. By modeling at the
level of the process, rather than the level of parametric equation, we
can more concisely capture the possible behavior of these curves. In
place of a parametric model of the dynamical system, the Gaussian
Process approach postulates a prior distribution of (n-dimensional)
curves that can be though of as approximations to a range of possible
(parametric) models that might describe the data. The GP allows us to
consider a set of possible curves simultaneously.\\

\subsubsection{The optimal control problem}

\subsubsection{Discussion on how we compare performance of policies}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Replicate stochastic simulations
\item
  Sensitivity analysis (Figure 4).
\end{itemize}

\subsection{Example in a bistable system}

Concerns over the potential for tipping points in ecological dynamics
(Scheffer et al. 2001) highlight the dangers of uncertainty in
ecological management and pose a substantial challenge to existing
decision-theoretic approaches (BrozoviÄ‡ and Schlenker 2011). To compare
the performance of nonparametric and parametric approaches in an example
that is easy to conceptualize, we will focus on a simple parametric
model for a single species (derived from fist principles by Allen et al.
2005) as our underlying ``reality''.

\begin{align}
X_{t+1} &= Z_t f(S_t) \\
S_t &= X_t - h_t \\
f(S_t) &= e^{r \left(1 - \frac{S_t}{K}\right)\left(S_t - C\right)}
\end{align}

Where $Z_t$ is multiplicative noise function with mean 1, representing
stochastic growth. We will consider log-normal noise with shape
parameter $\sigma_g$. We start with an example in which the parameters
are $r =$ 2, $K =$ 10, $C =$ 5, and $\sigma_g =$ 0.05.

As a low-dimensional system completely described by three parameters,
this scenario should if anything be favorable to a parametric-based
approach. This model contains an Allee effect, or tipping point, below
which the population is not self-sustaining and shrinks to zero
(Courchamp, Berec, and Gascoigne 2008).

\subsection{Sample training data}

Both parametric and nonparametric approaches will require training data
on which to base their model of the process. We generate the training
data under the model described in Eq 1 for 35 time steps, under a known
but not necessarily optimal sequence of harvest intensities, $h_t$. For
simplicity we imagine a fishery that started from zero harvest pressure
and has been gradually increasing the harvest.

Using data simulated from a specified model rather than empirical data
permits the comparison against the true underlying dynamics, setting a
bar for the optimal performance possible.

(Motivation, alternatives, stationarity, examples without a stable node
(limit-cycle models), examples based on observations near a stable node
alone, and why that isn't impossible).

\section{Results}

\subsubsection{Discussion of maximum likelihood estimated models}

We estimate two parametric models from the data using a maximum
likelihood approach. The first model is structurally identical to the
true model (Eq 1), differing only in that it's parameters are estimated
from the observed data rather than given. The alternative model is the
Ricker model, which is structurally similar and commonly used in for
such data.

(MLE models will assume the noise is log-normal, which it is in the
simulation).

Which estimates a Ricker model with $r =$ 1.8501, $K =$ 9.8091, and the
Allen Allele model with $r =$ 2.8079, $K =$ 11.8235 and $C =$ 7.2159.

\subsection{Figure 1:}

\emph{Shows the inferred Gaussian Process compared to the true and
parametric models. Refer to the appendix for details on the GP
posteriors, etc.}

\begin{figure}[htbp]
\centering
\includegraphics{figure/gp_plot.pdf}
\caption{Graph of the inferred Gaussian process compared to the true
process and maximum-likelihood estimated process. Graph shows the
expected value for the function $f$ under each model. Two standard
deviations from the estimated Gaussian process covariance with (light
grey) and without (darker grey) measurement error are also shown. The
training data is also shown as black points. (The GP is conditioned on
0,0, shown as a pseudo-data point).}
\end{figure}

\subsection{Figure 2:}

\emph{The take-home message, showing that the GP is closest to the
optimal strategy, while the parametric methods are less accurate.
Visualizing the policy may be more useful for the technical reader, the
general audience may prefer Figure 3 showing all replicates of the
population collapse under the parametric model and not under the GP.}

\begin{figure}[htbp]
\centering
\includegraphics{figure/policies_plot.pdf}
\caption{The steady-state optimal policy (infinite boundary) calculated
under each model. Policies are shown in terms of target escapement,
$S_t$, as under models such as this a constant escapement policy is
expected to be optimal (Reed 1979).}
\end{figure}

\subsection{Figure 3:}

\emph{Figure 3 is a less abstract and more visceral visualization of the
take-home message, with the structurally inaccurate model leading
universally to a collapse of the fishery and very few profits, while the
Gaussian process performs nearly optimally. The parametric approach even
with the correct underlying structure does not perform optimally,
choosing in this case to under-fish (may need to show harvest dynamics
since that is not clear from the figure! Also isn't general, sometimes
does optimally, sometimes over-fishes. Perhaps need to show more
examples.) May need to show profits too?}

\begin{figure}[htbp]
\centering
\includegraphics{figure/sim_plot.pdf}
\caption{Gaussian process inference outperforms parametric estimates.
Shown are 100 replicate simulations of the stock dynamics (eq 1) under
the policies derived from each of the estimated models, as well as the
policy based on the exact underlying model.}
\end{figure}

\subsection{Figure 4:}

\emph{Shows the sensitivity analysis. A histogram of distribution of
yield over stochastic realizations, showing that the qualitative results
do not depend on the stochastic realization of the training data here,
or on the parameters of the underlying model, though quantitative
differences are visible.}

\section{Discussion / Conclusion}

Non-parametric methods have received far too little attention in
ecological modeling efforts that are aimed at improved conservation
planning and decision making support.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Adapting a non-parametric approach requires modification of existing
  methods for decision theory. We have illustrated how this might be
  done in the context of stochastic dynamic programming, opening the
  door for substantial further research into how these applications
  might be improved.
\end{itemize}

Three elements contribute to the machine learning model performing as
well or better than the mechanisitic models in this context.

\subsubsection{1. Relevant state space}

The dynamics occur over a range of state-space in which the machine
learning model closely matches the predictions of y implausible model
performs as well or better than the more plausible model.

This is the most obvious and immediate reason why the shortcomings that
appear to make the model implausible do not make it useless. This effect
is enhanced in our example because the management actions help drive the
system towards rather than away from this region of state-space.

\subsubsection{2. Predictive accuracy}

This application relies only on the predictive accuracy of the model,
not an interpretation of the parameters.

Predictive accuracy is not the goal of all modeling, as ecologists have
been observing for as long as they made models (perhaps none more
memorably than @Levins1969). Mechanistic modeling is at its most
powerful not when it is used to directly forecast future states but when
it provides an understanding of how to approach a problem. SIR-type
models from the epidemiological literature are a good example. While the
simplest SIR models have little predictive power over the outbreak
intensity or timing at a particular location, they provide a powerful
understanding of the spread of an infection in terms of a single,
biologically meaningful parameter: $R_0$, the basic reproductive number.
From the model, it becomes clear that management need not vaccinate
every member of the population to stop the spread, but rather it
suffices to vaccinate a sufficient fraction of the population to reduce
$R_0$ below 1.

\subsubsection{3. Time scale for new data}

In the sequential decision making problem we considered, we are
presented with new data after each action. The relevant timescale for
the prediction is thus not the long-term dynamics, which would be wildly
divergent, but the dynamics over this much shorter interval. While
ecologists may be hesitant to base continual management on a model with
obviously inaccurate long-term behavior, engineers tend to consider the
problem in frequency space and gravitate to the opposite position -- a
good control model should prioritize high-frequency accuracy over low
frequency accuracy. The differences in intuition may arise from the
timescales at which each profession can typically adjust their control
variables -- much faster for a control system of a chemical plant than
state policy for a natural resource. Still, the lesson is clear: when
facing repeated management decisions over a short timescale, such as
setting annual harvests of a fishery, it may be more valuable to use a
machine learning algorithm that makes accurate year-ahead predictions
that capture some of the high-frequency fluctuations that appear only as
noise in a mechanistic model of the long-term population dynamics.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Better express ambiguity in regions where data is not available
\end{enumerate}

One of the greatest strengths of mechanistic models is their greatest
weakness as well.

\subsection{Future directions}

While we have highlighted certain generic of this problem that allow the
nonparametric approach to excel -- short timescales between successive
observations and actions, the accuracy in the appropriate region of
state space, the ability to express uncertainty outside the observed
data -- there are equally several aspects for which the nonparametric
approach is at a relative disadvantage.

In this simulated example, the underlying dynamics are truly governed by
a simple parametric model, allowing the parametric approaches to be more
accurate. Similarly, because the dynamics are one-dimensional dynamics
and lead to stable nodes (rather than other attractors such as
limit-cycles resulting in oscillations), the training data provides
relatively limited information about the dynamics. For these reasons, we
anticipate that in higher-dimensional examples characteristic of
ecosystem management problems that the machine learning approach will
prove even more valuable.

The machine learning approach is also better suited to complex and
disparate data. Incorporating various sources of information into
mechanistic models can be an immensely difficult due to the increased
complexity involved. Only thanks to tandem advances in increasing
computational power and hierarchical statistical methodology have we
been able to tackle such intuitively important complexity (and the
potentially new available data that accompanies it) such as spatial
distribution, heterogeneities of space, time, and individuals, to shift
to ecosystem-based approaches from single-species based approaches.
Without the need to formulate mechanisms, many modern machine learning
algorithms can leverage potential information from all available sources
of data directly. The algorithms can recognize unanticipated or subtle
patterns in large data sets that enable more accurate predictions than
mechanistic models that are formulated at a more macroscopic level.

\begin{center}\rule{3in}{0.4pt}\end{center}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Multiple species
\item
  Online learning
\item
  Multiple step-ahead predictions
\item
  Explicitly accomidating additional uncertainties
\item
  Improving inference of optimal policy from the GP
\end{itemize}

\section{Acknowledgments}

This work was partially supported by the Center for Stock Assessment
Research, a partnership between the University of California Santa Cruz
and the Fisheries Ecology Division, Southwest Fisheries Science Center,
Santa Cruz, CA and by NSF~grant EF-0924195 to MM.

\section{Appendix / Supplementary Materials}

\subsubsection{The Guassian process}

The Gaussian process is defined by a covariance kernel. By requiring our
kernel to follow a generic functional form, we can compactly describe
the Gaussian process using only a handful of parameters (Table 1)

\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
parameter & interpretation
\\\noalign{\medskip}
\hline\noalign{\medskip}
$\sigma^2$ & The process noise (from the kernel)
\\\noalign{\medskip}
$\tau^2$ & Variance around the mean
\\\noalign{\medskip}
$\beta$\_0 & The mean is given by a linear model of slope $\beta$
\\\noalign{\medskip}
$d$ & The length-scale of the covariance function
\\\noalign{\medskip}
$n$ & The observation error
\\\noalign{\medskip}
\hline
\noalign{\medskip}
\caption{Table of parameters for the Gaussian process}
\end{longtable}

Rather than estimate values of these parameters directly, we take a
hierarchical approach of placing prior distributions on each. Following
@Gramarcy2005 we use a Gaussian prior on $\beta_0$, an inverse gamma
prior on $\sigma^2$ and $\tau^2$, a gamma prior on the observation noise
$n$, and exponential prior on the length-scale $d$.

\subsubsection{Formulating a dynamic programming solution}

The fishery management problem over an infinite time horizon can be
stated as:

\begin{align}
& \max_{ \{h_t\} \geq 0 } \mathbf{E} \lbrace \sum_0^\infty \delta^t \Pi(h_t) \rbrace \\
& \mathrm{s.t.}  \\
 & X_t = Z_t f\left(S_{t-1}\right) \\
 & S_t = X_t - h_t \\
 & X_t  \geq 0 
\end{align}

Where $\mathbf{E}$ is the expectation operator, $\delta$ the discount
rate, $\Pi(h_t)$ the profit expected from a harvest of $h_t$, and other
terms as in Eq. (1). For simplicity, we have assumed that profits depend
only on the chosen harvest; simplifying further we will usually consider
profits to be proportional to harvest, $\Pi(h_t) = h_t$.

Once the posterior Gaussian process (GP) has been estimated (e.g.~see
Munch et al. 2005), it is necessary to adapt it in place of the
parametric equation for the stochastic dynamic programming (SDP)
solution (see Mangel and Clark 1988 for a detailed description of
parametric SDP methods) to the optimal policy. The essense of the idea
is straight forward -- we will use the estimated GP in place of the
parametric growth function to determine the stochastic transition matrix
on which the SDP calculations are based.

The posterior Gaussian process is completely defined by the expected
value and covariance matrix at a defined set of training points. For
simplicty we will consider a these points to fall on a discrete, uniform
grid $x$ of 101 points from 0 to 15 (1.5 times the positive equilibrium
$K$). Again to keep things simple we will use this same grid
discritization for the parametric approach. Other options for choosing
the grid points, including collocation methods and functional basis
expansion (or even using Guassian processes in place of the discrete
optimization; an entirely different context in which GP can be used in
SDP, see {[}@Deisenroth2009{]}) could also be considered.

The transition matrix $\mathbf{F}$ is thus an 101 by 101 matrix for
which the ${i,j}$ entry gives the probability of transitioning into
state $x_i$ given that the system is in state $x_j$ in the previous
timestep. To generate the transition matrix based on the posterior GP,
we need only the expected values at each grid point and the
corresponding variances (the diagonal of the covariance matrix), as
shown in Figure 1. Given the mean at each gridpoint as the length 101
vector $E$ and variance $V$, the probability of transitioning from state
$x_i$ to state $x_j$ is simply
$\mathcal{N}\left(x_j \vert  \mu = E_i, \sigma = \sqrt{V_i}\right)$,
where $\mathcal{N}$ is the Normal density at $x_j$ with mean $\mu$ and
variance $\sigma^2$. Strictly speaking, the transition probability
should be calculated by integrating the normal density over the bin of
width $\Delta$ centered at $x_j$. For a sufficiently fine grid that
$f(x_j) \approx f(x_j + \Delta)$, it is sufficient to calculate the
density at $x_j$ and then row-normalize the transition matrix.

\subsection{Pseudocode for the determining the transtion matrix from the
GP}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for(h in h_grid)}
  \NormalTok{F_h =}\StringTok{ }\NormalTok{for(x_j in grid)}
          \NormalTok{for(i in }\DecValTok{1}\NormalTok{:N) }
            \KeywordTok{dnorm}\NormalTok{(x_j, mu[i]-h, V[i])}
\end{Highlighting}
\end{Shaded}

A transition matrix for each of the parametric models $f$ is calculated
using the log-normal density with mean $f(x_i)$ and log-variance as
estimated by maximum likelihood. From the discrete transition matrix we
may write down the Bellman recursion defining the the stochastic dynamic
programming iteration:

\begin{equation}
V_t(x_t) = \max_h \mathbf{E} \left( h_t + \delta V_{t+1}( Z_{t+1} f(x_t - h_t)) \right)
\end{equation}

where $V(x_t)$ is the value of being at state $x$ at time $t$, $h$ is
control (harvest level) chosen. Numerically, the maximization is
accomplished as follows. Consider the set of possible control values to
be the discrete 101 values corresponding the the grid of stock sizes.
Then for each $h_t$ there is a corresponding transition matrix
$\mathbf{F}_h$ determined as described above but with mean
$\mu = x_j - h_t$. Let $\vec{V_t}$ be the vector whose $i$th element
corresponds to the value of having stock $x_i$ at time $t$. Then let
$\Pi_h$ be the vector whose $i$th element indicates the profit from
harvesting at intensity $h_t$ given a population $x_i$ (e.g.
$\max(x_i, h_t)$ since one cannot harvest more fish then the current
population size). Then the Bellman recursion can be given in matrix form
as

\[V_{t} = \max_h \left( \Pi_{h_{t}} + \delta \mathbf{F}_h V_{t+1} \right)\]

where the sum is element by element and the expectation is computed by
the matrix multiplication $\mathbf{F} V_{t+1}$.

\subsubsection{Pseudocode for the Bellman iteration}

\begin{Shaded}
\begin{Highlighting}[]
 \NormalTok{V1 <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(h_grid), function(h)\{}
      \NormalTok{delta *}\StringTok{ }\NormalTok{F[[h]] %*%}\StringTok{ }\NormalTok{V +}\StringTok{  }\KeywordTok{profit}\NormalTok{(x_grid, h_grid[h]) }
    \NormalTok{\})}
    \CommentTok{# find havest, h that gives the maximum value}
    \NormalTok{out <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\NormalTok{:gridsize, function(j)\{}
      \NormalTok{value <-}\StringTok{ }\KeywordTok{max}\NormalTok{(V1[j,], }\DataTypeTok{na.rm =} \NormalTok{T) }\CommentTok{# each col is a diff h, max over these}
      \NormalTok{index <-}\StringTok{ }\KeywordTok{which.max}\NormalTok{(V1[j,])  }\CommentTok{# store index so we can recover h's }
      \KeywordTok{c}\NormalTok{(value, index) }\CommentTok{# returns both profit value & index of optimal h.  }
    \NormalTok{\})}
    \CommentTok{# Sets V[t+1] = max_h V[t] at each possible state value, x}
    \NormalTok{V <-}\StringTok{ }\NormalTok{out[}\DecValTok{1}\NormalTok{,]                        }\CommentTok{# The new value-to-go}
    \NormalTok{D[,OptTime-time}\DecValTok{+1}\NormalTok{] <-}\StringTok{ }\NormalTok{out[}\DecValTok{2}\NormalTok{,]       }\CommentTok{# The index positions}
\end{Highlighting}
\end{Shaded}

\emph{Currently this shows the literal R code, should be adapted}

\subsection{MCMC posterior distributions and convergence analysis}

\begin{figure}[htbp]
\centering
\includegraphics{figure/posteriors.pdf}
\caption{Histogram of posterior distributions for the estimated Gaussian
Process shown in Figure 1. Prior distributions overlaid.}
\end{figure}

@Gramacy2005

\subsection{Tables of nuisance parameters, sensitivity analysis}

\subsubsection{List of hyper-parameters, prior distributions and their
parameters}

\subsection{Reproducible code, ``Research Compendium''}

Allen, Linda J. S., Jesse F. Fagan, GÃ¶ran HÃ¶gnÃ¤s, and Henrik Fagerholm.
2005. ``Population extinction in discrete-time stochastic population
models with an Allee effect.'' \emph{Journal of Difference Equations and
Applications} 11 (4-5) (apr): 273--293.
doi:10.1080/10236190412331335373.

BrozoviÄ‡, Nicholas, and Wolfram Schlenker. 2011. ``Optimal management of
an ecosystem with an unknown threshold.'' \emph{Ecological Economics}
(jan): 1--14. doi:10.1016/j.ecolecon.2010.10.001.

Courchamp, Franck, Ludek Berec, and Joanna Gascoigne. 2008. \emph{Allee
Effects in Ecology and Conservation}. Oxford University Press, USA.

Cressie, Noel, Catherine a Calder, James S. Clark, Jay M. Ver Hoef, and
Christopher K. Wikle. 2009. ``Accounting for uncertainty in ecological
analysis: the strengths and limitations of hierarchical statistical
modeling.'' \emph{Ecological Applications} 19 (3) (apr): 553--70.

Levins, Richard. 1966. ``The strategy of model building in population
biology.'' \emph{American Scientist} 54 (4): 421--431.

Mangel, Marc, and Colin W. Clark. 1988. \emph{Dynamic Modeling in
Behavioral Ecology}. Princeton: Princeton University Press.

Munch, Stephan B., Melissa L. Snover, George M. Watters, and Marc
Mangel. 2005. ``A unified treatment of top-down and bottom-up control of
reproduction in populations.'' \emph{Ecology Letters} 8 (7) (may):
691--695. doi:10.1111/j.1461-0248.2005.00766.x.

Reed, William J. 1979. ``Optimal escapement levels in stochastic and
deterministic harvesting models.'' \emph{Journal of Environmental
Economics and Management} 6 (4) (dec): 350--363.
doi:10.1016/0095-0696(79)90014-7.

Scheffer, Marten, Stephen R. Carpenter, J. A. Foley, C. Folke, and B.
Walker. 2001. ``Catastrophic shifts in ecosystems.'' \emph{Nature} 413
(6856) (oct): 591--6. doi:10.1038/35098000.

\end{document}


