---
layout: 12pt,review
linenumbers: true
title: "Avoiding tipping points in fisheries management: A Bayesian Non-Parametric approach"
author: 
  - name: Carl Boettiger
    affiliation: cstar
    email: cboettig(at)gmail.com
    footnote: Corresponding author
  - name: Marc Mangel
    affiliation: cstar
  - name: Stephan Munch
    affiliation: noaa
address: 
  - code: cstar
    address: | 
      Center for Stock Assessment Research, 
      Department of Applied Math and Statistics, 
      University of California, Mail Stop SOE-2,
      Santa Cruz, CA 95064, USA
  - code: marc
    address: |
      Center for Stock Assessment Research, Department of Applied Math
      and Statistics, University of California,
      Mail Stop SOE-2, Santa Cruz, CA 95064, USA and Department of
      Biology, University of Bergen, Bergen, Norway 9020
  - code: noaa
    address: | 
      Southwest Fisheries Science Center, 
      National Oceanic and Atmospheric Administration, 
      110 Shaffer Road, Santa Cruz, CA 95060, USA

abstract: |
          Model uncertainty and limited data coverage are fundamental
          challenges to robust management of human intervention in a 
          natural system.  These challenges
          are acutely highlighted by concerns that many ecological systems
          may contain tipping points, such as Allee population sizes.  
          Before a collapse, we do not know
          where the tipping points lie, if they exist at all.  Hence,
          we know neither a complete model of the system dynamics nor
          do we have access to data in some large region of state-space
          where such a tipping point might exist.  We illustrate
          how a Bayesian Non-Parametric (BNP) approach using a Gaussian Process (GP) prior
          provides a flexible representation of this inherent
          uncertainty. We embed GPs in a
          stochastic dynamic programming framework in order to make
          robust management predictions with both model uncertainty and
          limited data. We conduct a set of simulations to evaluate this
          approach as compared with the standard approach of using model
          selection to choose from a set of candidate models.  We find that model selection erroneously
          favors models without tipping points -- leading to harvest policies
          that guarantee extinction. The GP approach
          performs nearly as well as the true model and significantly
          outperforms standard appraoches. We illustrate this using examples
          of simulated single-species dynamics, where the standard model
          selection approach should be most effective, and find that it
          still fails to account for uncertainty appropriately and leads
          to population crashes, while management based on the GP 
          approach does not, since it does not underestimate the uncertainty
          outside of the observed data.   

keywords:
  - Bayesian
  - Structural Uncertainty
  - Nonparametric 
  - Optimal Control
  - Decision Theory
  - Gaussian Processes
  - Fisheries Management

bibliography: components/references.bib
csl: components/ecology.csl
documentclass: components/elsarticle

## rmarkdown render options
output:
  pdf_document:
    fig_caption: true
    template: components/elsarticle.latex

---


```{r caching, include=FALSE}
library("methods")
library("knitr")
basename <- "manuscript"
opts_chunk$set(fig.path = paste("components/figure/", basename, "-", sep=""),
               cache.path = paste("components/cache/", basename, "/", sep=""))
opts_chunk$set(cache = 2)
opts_chunk$set(tidy=FALSE, warning=FALSE, message=FALSE, 
               comment = NA, verbose = TRUE, echo=FALSE)

# PDF-based figures
opts_chunk$set(dev='pdf')
```


<!-- Run R code for analysis, to be called by figures -->
```{r plot-options, message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```

```{r libraries, cache=FALSE, message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
library("nonparametricbayes") 
library("pdgControl")

library("ggplot2") 
library("reshape2")
library("plyr")
library("data.table")

library("R2jags")
library("emdbook") # for as.mcmc.bugs (?)
library("coda")  # for as.mcmc

library("modeest")
library("MASS")
library("pander")

opts_chunk$set(fig.width=4, fig.height=3, echo=FALSE)
theme_set(theme_bw(base_size=12))
toggle = "hide" # results argument
```


```{r posterior-mode}
library(modeest)
posterior.mode <- function(x) {
  mlv(x, method="shorth")$M
}
```
```{r stateeq}
f <- RickerAllee
p <- c(2, 8, 5)
K <- 10  # approx, a li'l' less
allee <- 5 # approx, a li'l' less
```
```{r sdp-pars, dependson="stateeq"}
sigma_g <- 0.05
sigma_m <- 0.0
z_g <- function() rlnorm(1, 0, sigma_g)
z_m <- function() 1
x_grid <- seq(0, 1.5 * K, length=50)
h_grid <- x_grid
profit <- function(x,h) pmin(x, h)
delta <- 0.01
OptTime <- 50  # stationarity with unstable models is tricky thing
reward <- 0
xT <- 0
Xo <-  allee+.5# observations start from
x0 <- K # simulation under policy starts from
Tobs <- 40
MaxT = 1000 # timeout for value iteration convergence
```
```{r obs, dependson="sdp-pars", fig.keep='none'}
  set.seed(1234)
  #harvest <- sort(rep(seq(0, .5, length=7), 5))
  x <- numeric(Tobs)
  x[1] <- Xo
  nz <- 1
  for(t in 1:(Tobs-1))
    x[t+1] = z_g() * f(x[t], h=0, p=p)
  obs <- data.frame(x = c(rep(0,nz), 
                          pmax(rep(0,Tobs-1), x[1:(Tobs-1)])), 
                    y = c(rep(0,nz), 
                          x[2:Tobs]))
raw_plot <- ggplot(data.frame(time = 1:Tobs, x=x), aes(time,x)) + geom_line()
```


```{r mle, dependson="obs"}
set.seed(12345)
estf <- function(p){ 
    mu <- f(obs$x,0,p)
    -sum(dlnorm(obs$y, log(mu), p[4]), log=TRUE)
}
par <- c(p[1]*rlnorm(1,0,.1), 
         p[2]*rlnorm(1,0,.1), 
         p[3]*rlnorm(1,0, .1), 
         sigma_g * rlnorm(1,0,.1))
o <- optim(par, estf, method="L", lower=c(1e-5,1e-5,1e-5,1e-5))
f_alt <- f
p_alt <- c(as.numeric(o$par[1]), as.numeric(o$par[2]), as.numeric(o$par[3]))
sigma_g_alt <- as.numeric(o$par[4])

est <- list(f = f_alt, p = p_alt, sigma_g = sigma_g_alt, mloglik=o$value)

```
```{r mle-output, dependson="mle", results=toggle}
true_means <- sapply(x_grid, f, 0, p)
est_means <- sapply(x_grid, est$f, 0, est$p)
```


```{r gp-priors}
s2.p <- c(5,5)  
d.p = c(10, 1/0.1)
```
```{r gp, dependson=c("gp-priors", "obs")}
gp <- gp_mcmc(obs$x, y=obs$y, n=1e5, s2.p = s2.p, d.p = d.p)
gp_dat <- gp_predict(gp, x_grid, burnin=1e4, thin=300)
```
```{r gp_traces_densities, dependson="gp"}
gp_assessment_plots <- summary_gp_mcmc(gp, burnin=1e4, thin=300)
```
```{r gp-output, dependson="gp", results=toggle}
tgp_dat <- 
    data.frame(  x = x_grid, 
                 y = gp_dat$E_Ef, 
                 ymin = gp_dat$E_Ef - 2 * sqrt(gp_dat$E_Vf), 
                 ymax = gp_dat$E_Ef + 2 * sqrt(gp_dat$E_Vf) )
```

```{r jags-setup}
y <- x 
N <- length(x);
jags.data <- list("N","y")
n.chains <- 6
n.iter <- 1e6
n.burnin <- floor(10000)
n.thin <- max(1, floor(n.chains * (n.iter - n.burnin)/1000))
n.update <- 10

```
```{r common-priors}
stdQ_prior_p <- c(1e-6, 100)
stdR_prior_p <- c(1e-6, .1)
stdQ_prior  <- function(x) dunif(x, stdQ_prior_p[1], stdQ_prior_p[2])
stdR_prior  <- function(x) dunif(x, stdR_prior_p[1], stdR_prior_p[2])

```
```{r allen-model}
K_prior_p <- c(0.01, 20.0)
r0_prior_p <- c(0.01, 6.0)
theta_prior_p <- c(0.01, 20.0)

bugs.model <- 
paste(sprintf(
"model{
  K     ~ dunif(%s, %s)
  r0    ~ dunif(%s, %s)
  theta ~ dunif(%s, %s)
  stdQ ~ dunif(%s, %s)", 
  K_prior_p[1], K_prior_p[2],
  r0_prior_p[1], r0_prior_p[2],
  theta_prior_p[1], theta_prior_p[2],
  stdQ_prior_p[1], stdQ_prior_p[2]),

  "
  iQ <- 1 / (stdQ * stdQ);
  y[1] ~ dunif(0, 10)
  for(t in 1:(N-1)){
    mu[t] <- log(y[t]) + r0 * (1 - y[t]/K)* (y[t] - theta) / K 
    y[t+1] ~ dlnorm(mu[t], iQ) 
  }
}")
writeLines(bugs.model, "allen_process.bugs")

```
```{r allen-priors, dependson="common-priors"}
K_prior     <- function(x) dunif(x, K_prior_p[1], K_prior_p[2])
r0_prior <- function(x) dunif(x, r0_prior_p[1], r0_prior_p[2])
theta_prior <- function(x) dunif(x, theta_prior_p[1], theta_prior_p[2])
par_priors  <- list(K = K_prior, deviance = function(x) 0 * x, 
                    r0 = r0_prior, theta = theta_prior,
                    stdQ = stdQ_prior)

```

```{r allen_priors_xtable, include=TRUE, echo=FALSE}
allen_priors_xtable <- data.frame(parameter = c("$r$", "$K$", "$X_C$", "$\\sigma$"),
           "lower bound" = c(r0_prior_p[1], K_prior_p[1], theta_prior_p[1], stdQ_prior_p[1]),
           "upper bound" = c(r0_prior_p[2], K_prior_p[2], theta_prior_p[2], stdQ_prior_p[2]))
```

```{r allen-mcmc, dependson=c("allen-model", "allen-pars", "jags-setup"), results=toggle}
jags.params=c("K","r0","theta","stdQ") # be sensible about the order here
jags.inits <- function(){
  list("K"= 10 * rlnorm(1,0, 0.1),
       "r0"= 1 * rlnorm(1,0, 0.1) ,
       "theta"=   5 * rlnorm(1,0, 0.1) , 
       "stdQ"= abs( 0.1 * rlnorm(1,0, 0.1)),
       .RNG.name="base::Wichmann-Hill", .RNG.seed=123)
}

set.seed(1234)
# parallel refuses to take variables as arguments (e.g. n.iter = 1e5 works, but n.iter = n doesn't)
allen_jags <- do.call(jags, list(data=jags.data, inits=jags.inits, 
                                      jags.params, n.chains=n.chains, 
                                      n.iter=n.iter, n.thin=n.thin, 
                                      n.burnin=n.burnin, 
                                      model.file="allen_process.bugs"))

# Run again iteratively if we haven't met the Gelman-Rubin convergence criterion
recompile(allen_jags) # required for parallel
allen_jags <- do.call(autojags, 
											list(object=allen_jags, n.update=n.update, 
                           n.iter=n.iter, n.thin = n.thin))


```
```{r allen-traces, dependson="allen-mcmc"}
tmp <- lapply(as.mcmc(allen_jags), as.matrix) # strip classes the hard way...
allen_posteriors <- melt(tmp, id = colnames(tmp[[1]])) 
names(allen_posteriors) = c("index", "variable", "value", "chain")
plot_allen_traces <- ggplot(allen_posteriors) + geom_line(aes(index, value)) + 
  facet_wrap(~ variable, scale="free", ncol=1)

```
```{r allen-posteriors, dependson=c("allen-traces", "allen-priors")}
allen_priors <- ddply(allen_posteriors, "variable", function(dd){
    grid <- seq(min(dd$value), max(dd$value), length = 100) 
    data.frame(value = grid, density = par_priors[[dd$variable[1]]](grid))
})
plot_allen_posteriors <- ggplot(allen_posteriors, aes(value)) + 
  stat_density(geom="path", position="identity", alpha=0.7) +
#  geom_line(data=allen_priors, aes(x=value, y=density), col="red") +  
  facet_wrap(~ variable, scale="free", ncol=3)

```
```{r allen-output, dependson=c("posterior-mode", "allen-traces"), results=toggle}
A <- allen_posteriors
A$index <- A$index + A$chain * max(A$index) # Combine samples across chains by renumbering index 
pardist <- acast(A, index ~ variable)
bayes_coef <- apply(pardist,2, posterior.mode) 
bayes_pars <- unname(c(bayes_coef["r0"], bayes_coef["K"], bayes_coef["theta"])) # parameters formatted for f
allen_f <- function(x,h,p) unname(RickerAllee(x,h, unname(p[c("r0", "K", "theta")])))
allen_means <- sapply(x_grid, f, 0, bayes_pars)
bayes_pars
head(pardist)

```


```{r ricker-model}
K_prior_p <- c(0.01, 40.0)
r0_prior_p <- c(0.01, 20.0)
bugs.model <- 
paste(sprintf(
"model{
  K    ~ dunif(%s, %s)
  r0    ~ dunif(%s, %s)
  stdQ ~ dunif(%s, %s)", 
  K_prior_p[1], K_prior_p[2],
  r0_prior_p[1], r0_prior_p[2],
  stdQ_prior_p[1], stdQ_prior_p[2]),
  "
  iQ <- 1 / (stdQ * stdQ);
  y[1] ~ dunif(0, 10)
  for(t in 1:(N-1)){
    mu[t] <- log(y[t]) + r0 * (1 - y[t]/K) 
    y[t+1] ~ dlnorm(mu[t], iQ) 
  }
}")
writeLines(bugs.model, "ricker_process.bugs")

```
```{r ricker-priors, dependson="common-priors"}
K_prior     <- function(x) dunif(x, K_prior_p[1], K_prior_p[2])
r0_prior <- function(x) dunif(x, r0_prior_p[1], r0_prior_p[2])
par_priors <- list(K = K_prior, deviance = function(x) 0 * x, 
                   r0 = r0_prior, stdQ = stdQ_prior)

```

```{r ricker_priors_xtable, include=TRUE, echo=FALSE, cache=FALSE}
ricker_priors_xtable <- data.frame(
  parameter = c("$r$", "$K$", "$\\sigma$"),
  "lower bound" = c(r0_prior_p[1], K_prior_p[1], stdQ_prior_p[1]),
  "upper bound" = c(r0_prior_p[2], K_prior_p[2], stdQ_prior_p[2]))
```

```{r ricker-mcmc, dependson="ricker-model", results=toggle}
jags.params=c("K","r0", "stdQ")
jags.inits <- function(){
  list("K"= 10 * rlnorm(1,0,.5),
       "r0"= rlnorm(1,0,.5),
       "stdQ"=sqrt(0.05) * rlnorm(1,0,.5),
       .RNG.name="base::Wichmann-Hill", .RNG.seed=123)
}
set.seed(12345) 
ricker_jags <- do.call(jags, 
                       list(data=jags.data, inits=jags.inits, 
                            jags.params, n.chains=n.chains, 
                            n.iter=n.iter, n.thin=n.thin, n.burnin=n.burnin,
                            model.file="ricker_process.bugs"))
recompile(ricker_jags)
ricker_jags <- do.call(autojags, 
                       list(object=ricker_jags, n.update=n.update, 
														n.iter=n.iter, n.thin = n.thin, 
														progress.bar="none"))

```
```{r ricker-traces, dependson="ricker-mcmc"}
tmp <- lapply(as.mcmc(ricker_jags), as.matrix) # strip classes the hard way...
ricker_posteriors <- melt(tmp, id = colnames(tmp[[1]])) 
names(ricker_posteriors) = c("index", "variable", "value", "chain")
plot_ricker_traces <- ggplot(ricker_posteriors) + geom_line(aes(index, value)) + 
  facet_wrap(~ variable, scale="free", ncol=1)
```
```{r ricker-posteriors, dependson=c("ricker-traces", "ricker-priors")}
ricker_priors <- ddply(ricker_posteriors, "variable", function(dd){
    grid <- seq(min(dd$value), max(dd$value), length = 100) 
    data.frame(value = grid, density = par_priors[[dd$variable[1]]](grid))
})
# plot posterior distributions
plot_ricker_posteriors <- ggplot(ricker_posteriors, aes(value)) + 
  stat_density(geom="path", position="identity", alpha=0.7) +
#  geom_line(data=ricker_priors, aes(x=value, y=density), col="red") +  # don't plot priors 
  facet_wrap(~ variable, scale="free", ncol=2)
```
```{r ricker-output, dependson=c("posterior-mode", "ricker-traces"), results=toggle}
A <- ricker_posteriors
A$index <- A$index + A$chain * max(A$index) # Combine samples across chains by renumbering index 
ricker_pardist <- acast(A, index ~ variable)
bayes_coef <- apply(ricker_pardist,2, posterior.mode) 
ricker_bayes_pars <- unname(c(bayes_coef["r0"], bayes_coef["K"]))
ricker_f <- function(x,h,p){
  sapply(x, function(x){ 
    x <- pmax(0, x-h) 
    pmax(0, x * exp(p["r0"] * (1 - x / p["K"] )) )
  })
}
ricker_means <- sapply(x_grid, Ricker, 0, ricker_bayes_pars[c(1,2)])
head(ricker_pardist)
ricker_bayes_pars

```


```{r myers-model}
r0_prior_p <- c(.0001, 10.0)
theta_prior_p <- c(.0001, 10.0)
K_prior_p <- c(.0001, 40.0)
bugs.model <- 
paste(sprintf(
"model{
  r0    ~ dunif(%s, %s)
  theta    ~ dunif(%s, %s)
  K    ~ dunif(%s, %s)
  stdQ ~ dunif(%s, %s)", 
  r0_prior_p[1], r0_prior_p[2],
  theta_prior_p[1], theta_prior_p[2],
  K_prior_p[1], K_prior_p[2],
  stdQ_prior_p[1], stdQ_prior_p[2]),

  "
  iQ <- 1 / (stdQ * stdQ);

  y[1] ~ dunif(0, 10)
  for(t in 1:(N-1)){
    mu[t] <- log(r0)  + theta * log(y[t]) - log(1 + pow(abs(y[t]), theta) / K)
    y[t+1] ~ dlnorm(mu[t], iQ) 
  }
}")
writeLines(bugs.model, "myers_process.bugs")

```
```{r myers-priors}
K_prior     <- function(x) dunif(x, K_prior_p[1], K_prior_p[2])
r_prior     <- function(x) dunif(x, r0_prior_p[1], r0_prior_p[2])
theta_prior <- function(x) dunif(x, theta_prior_p[1], theta_prior_p[2])
par_priors <- list( deviance = function(x) 0 * x, K = K_prior,
                    r0 = r_prior, theta = theta_prior, 
                    stdQ = stdQ_prior)

```

```{r myers_prior_xtable, include=TRUE, echo=FALSE} 
myers_priors_xtable <- data.frame(parameter = c("$r$", "$K$", "$\\theta$", "$\\sigma$"),
           "lower bound" = c(r0_prior_p[1], K_prior_p[1], theta_prior_p[1], stdQ_prior_p[1]),
           "upper bound" = c(r0_prior_p[2], K_prior_p[2], theta_prior_p[2], stdQ_prior_p[2]))
```


```{r myers-mcmc, dependson="myers-model", results=toggle}
jags.params=c("r0", "theta", "K", "stdQ")
jags.inits <- function(){
  list("r0"= 1 * rlnorm(1,0,.1), 
       "K"=    10 * rlnorm(1,0,.1),
       "theta" = 1 * rlnorm(1,0,.1),  
       "stdQ"= sqrt(0.2) * rlnorm(1,0,.1),
       .RNG.name="base::Wichmann-Hill", .RNG.seed=123)
}
set.seed(12345)
myers_jags <- do.call(jags, 
                      list(data=jags.data, inits=jags.inits, 
													 jags.params, n.chains=n.chains, 
													 n.iter=n.iter, n.thin=n.thin,
                           n.burnin=n.burnin, 
                           model.file="myers_process.bugs"))
recompile(myers_jags)
myers_jags <- do.call(autojags, 
                      list(myers_jags, n.update=n.update, 
                           n.iter=n.iter, n.thin = n.thin, 
                           progress.bar="none"))

```
```{r myers-traces, dependson="myers-mcmc"}
tmp <- lapply(as.mcmc(myers_jags), as.matrix) # strip classes
myers_posteriors <- melt(tmp, id = colnames(tmp[[1]])) 
names(myers_posteriors) = c("index", "variable", "value", "chain")
plot_myers_traces <- ggplot(myers_posteriors) + 
  geom_line(aes(index, value)) + # priors, need to fix order though
  facet_wrap(~ variable, scale="free", ncol=1)

```
```{r myers-posteriors, dependson="myers-traces"}
par_prior_curves <- ddply(myers_posteriors, "variable", function(dd){
    grid <- seq(min(dd$value), max(dd$value), length = 100) 
    data.frame(value = grid, density = par_priors[[dd$variable[1]]](grid))
})
plot_myers_posteriors <- ggplot(myers_posteriors, aes(value)) + 
  stat_density(geom="path", position="identity", alpha=0.7) +
#  geom_line(data=par_prior_curves, aes(x=value, y=density), col="red") +  # Whoops, these are misaligned. see table instead 
  facet_wrap(~ variable, scale="free", ncol=3)

```
```{r myers-output, dependson=c("posterior-mode", "myers-traces"), results=toggle}
A <- myers_posteriors
A$index <- A$index + A$chain * max(A$index) # Combine samples across chains by renumbering index 
myers_pardist <- acast(A, index ~ variable)
bayes_coef <- apply(myers_pardist,2, posterior.mode) # much better estimates
myers_bayes_pars <- unname(c(bayes_coef["r0"], bayes_coef["theta"], bayes_coef["K"]))
myers_means <- sapply(x_grid, Myer_harvest, 0, myers_bayes_pars)
myers_f <- function(x,h,p) Myer_harvest(x, h, p[c("r0", "theta", "K")])
head(myers_pardist)
myers_bayes_pars

```


```{r assemble-models, dependson=c("myers-output", "ricker-output", "allen-output", "gp-output", "mle-output")}
models <- data.frame(x=x_grid, 
										 GP=tgp_dat$y, 
										 True=true_means, 
                     MLE=est_means, 
										 Ricker=ricker_means, 
                     Allen = allen_means,
                     Myers = myers_means)
models <- melt(models, id="x")
# some labels
names(models) <- c("x", "method", "value")
# labels for the colorkey too
model_names = c("GP", "True", "MLE", "Ricker", "Allen", "Myers")
colorkey=cbPalette
names(colorkey) = model_names 
step_ahead <- function(x, f, p){
  h = 0
  x_predict <- sapply(x, f, h, p)
  n <- length(x_predict) - 1
  y <- c(x[1], x_predict[1:n])
  y
}
```
```{r gp-opt, dependson="gp-output"}
matrices_gp <- gp_transition_matrix(gp_dat$Ef_posterior, gp_dat$Vf_posterior, x_grid, h_grid) 
opt_gp <- value_iteration(matrices_gp, x_grid, h_grid, MaxT, xT, profit, delta, reward)
```
```{r mle-opt, dependson="mle-output"}
matrices_true <- f_transition_matrix(f, p, x_grid, h_grid, sigma_g)
opt_true <- value_iteration(matrices_true, x_grid, h_grid, OptTime=MaxT, xT, profit, delta=delta)
matrices_estimated <- f_transition_matrix(est$f, est$p, x_grid, h_grid, est$sigma_g)
opt_estimated <- value_iteration(matrices_estimated, x_grid, h_grid, OptTime=MaxT, xT, profit, delta=delta)

```
```{r allen-opt, dependson="allen-output"}
matrices_allen <- parameter_uncertainty_SDP(allen_f, x_grid, h_grid, pardist, 4)
opt_allen <- value_iteration(matrices_allen, x_grid, h_grid, OptTime=MaxT, xT, profit, delta=delta)
```
```{r ricker-opt, dependson="ricker-output"}
matrices_ricker <- parameter_uncertainty_SDP(ricker_f, x_grid, h_grid, as.matrix(ricker_pardist), 3)
opt_ricker <- value_iteration(matrices_ricker, x_grid, h_grid, OptTime=MaxT, xT, profit, delta=delta)
```
```{r myers-opt, dependson="myers-output"}
matrices_myers <- parameter_uncertainty_SDP(myers_f, x_grid, h_grid, as.matrix(myers_pardist), 4)
myers_alt <- value_iteration(matrices_myers, x_grid, h_grid, OptTime=MaxT, xT, profit, delta=delta)

```
```{r assemble-opt, dependson=c("gp-opt", "mle-opt", "allen-opt", "ricker-opt", "myers-opt")}
OPT = data.frame(GP = opt_gp$D, True = opt_true$D, MLE = opt_estimated$D, Ricker = opt_ricker$D, Allen = opt_allen$D, Myers = myers_alt$D)
colorkey=cbPalette
names(colorkey) = names(OPT) 
```
```{r sims, dependson="assemble-opt"}
sims <- lapply(OPT, function(D){
  set.seed(1)
  lapply(1:100, function(i) 
    ForwardSimulate(f, p, x_grid, h_grid, x0, D, z_g, profit=profit, OptTime=OptTime)
  )
})
# turn the list into a data.frame
keep.as.columns <- names(sims[[1]][[1]])
dat <- melt(sims, id=keep.as.columns)
sims_data <- data.table(dat)
setnames(sims_data, c("L1", "L2"), c("method", "reps")) 
# Legend in original ordering please, not alphabetical: 
sims_data$method = factor(sims_data$method, ordered=TRUE, levels=names(OPT))
```


```{r profits, dependson="sims", results=toggle}
Profit <- sims_data[, sum(profit), by=c("reps", "method")]
tmp <- dcast(Profit, reps ~ method)
#tmp$Allen <- tmp[,"Allen"] + rnorm(dim(tmp)[1], 0, 1) # jitter for plotting
tmp <- tmp / tmp[,"True"]
tmp <- melt(tmp[2:dim(tmp)[2]])
actual_over_optimal <-subset(tmp, variable != "True")
```

```{r dic_calc, dependson=c("posterior-mode", "myers-output", "ricker-output", "allen-output", "gp-output", "mle-output"), include=FALSE, echo=FALSE}
dic.dic <- function (x) sum(x$deviance) +  sum(x[[2]])
recompile(allen_jags)
allen_dic <- dic.dic(dic.samples(allen_jags$model, n.iter=1000, type="popt"))
recompile(ricker_jags)
ricker_dic <- dic.dic(dic.samples(ricker_jags$model, n.iter=1000, type="popt"))
recompile(myers_jags)
myers_dic <- dic.dic(dic.samples(myers_jags$model, n.iter=1000, type="popt"))
dictable <- data.frame(Allen = allen_dic + 2*length(bayes_pars), 
                       Ricker = ricker_dic + 2*length(ricker_bayes_pars),
                       Myers = myers_dic + 2*length(myers_bayes_pars), 
                       row.names = c("DIC"))
```


```{r sensitivity-trends}
source("components/sensitivity-trends.R") # load the function for looping

sigmas <- c(0.01, 0.05, 0.1)
allee <- c(1,2,3,4,5)

vary_sigma <- lapply(sigmas, function(s) try(sensitivity_trends(4, sigma = s, seed=c(111,222,333))))
vary_allee <-  lapply(allee, function(a) try(sensitivity_trends(a, sigma = sigma_g, seed=c(111,222,333))))

vary_sigma <- melt(vary_sigma, id=names(vary_sigma[[1]]))
vary_allee <- melt(vary_allee, id=names(vary_allee[[1]]))

```




Introduction
============

Decision making under uncertainty is a ubiquitous challenge in the
management of human intervention in natural resources and conservation.
Ecological dynamics are frequently complex and difficult to measure,
making uncertainty in our understanding a persistent challenge.
Decision-theoretic approaches provide a framework to determine the
best sequence of actions in face of uncertainty, but only when that
uncertainty can be meaningfully quantified [@Fischer2009].  Over the last
four decades (but stretching even further back in time), marked by  the
seminal contributions of @Clark1976; @Clark2009 and @Walters1978,
dynamic optimization methods, particularly Stochastic Dynamic Programming
(SDP, @Clark2009), have become increasingly important as a means of
understanding how to management human intervention into natural systems.
Concomittantly, there has been increasing recognition of the importance of
Allee population sizes or `tipping points' [@Scheffer2001;
@Polasky2011] that management actions may inadvertently
cause populations to cross.  @Scheffer2009 emphasized the difficulties of
formulating even qualitatively correct models of the underlying processes.


We develop a novel approach to address these concerns in the context of
fisheries; though the underlying challenges and methods are germane to
many other problems of conservation or natural resource exploitation,
such as forestry.  Economic value and ecological concern have made
marine fisheries the crucible for much of the founding work [@Gordon1954;
@Clark1976; -@Clark2009; @Reed1979; @May1979; @Ludwig1982] on management
under uncertainty.  Global declines [@Worm2006] and the controversy
surrounding their interpretation [@Hilborn2007; @Worm2009] have made
understanding these challenges all the more pressing.

<!-- Uncertainty outside the data without the correct model has not been handled. -->
<!-- __We don't have the model__ -->

Uncertainty enters the decision-making process at many levels:
intrinsic stochasticity in biological processes, measurements, and
implementation of policy [_e.g._ @Reed1979; @Clark1986; @Roughgarden1996;
@Sethi2005], parametric uncertainty [_e.g._ @Ludwig1982; @Hilborn1997;
@McAllister1998; @Schapaugh2013], and model or structural uncertainty
[_e.g._ @Williams2001; @Cressie2009;  @Athanassoglou2012].  Of these,
structural uncertainty is generally the hardest to quantify. Typical approaches
assume a weak notion of model uncertainty in which the correct model
(or reasonable approximation) of the dynamics must be identified from
among a handful of alternative models, using either model choice, model averaging, or 
introducing yet greater model complexity of which others may be special
cases (model averaging being one such way to construct such a model)
[@Williams2001; @Athanassoglou2012; @Cressie2009].  Even setting aside
other computational and statistical concerns (e.g. @Cressie2009), these
approaches do not address our second concern - representing uncertainty
outside the observed data range.


<!-- __We don't have the data where we need it__ -->
<!-- What do we call this?  Extrapolation uncertainty?  Pathological Uncertainty? -->


Model uncertainty is particularly insidious when model predictions must
be made outside of the range of data on which the model was estimated, and models may be abused [@Mangel2001].
Extrapolation uncertainty is felt most keenly in decision-theoretic
(or optimal control) applications, since (a) exploring the potential
action space typically involves considering actions that may move the
system outside the range of observed behavior, and (b) decision-theoretic
algorithms rely not only on reasonable estimates of the expected
outcomes, but depend on the weights given to all possible outcomes
[_e.g._ @Weitzman2013].  


<!--This difficult position of having neither the true model nor data that
covers the full range of possible states is unfortunately the rule more
than the exception. The potential concern of tipping points in ecological
dynamics [@Scheffer2001; @Polasky2011] reflects these concerns -- as
either knowledge of the true model or more complete sampling of the
state space would make it easy to identify if a tipping point existed.
If we do not know but cannot rule out such a possibility, then we
face decision-making under this 
-->


The dual concerns of model uncertainty and
incomplete data coverage pose a substantial challenge to existing decision-theoretic
approaches [@Brozovic2011].  Because intervention is often (but not always, see @Hughes2013) too late after a tipping point has been crossed, management
is most often concerned with avoiding potentially catastrophic tipping
points before any data is available at or following a transition that
would more clearly reveal these regime shift dynamics [e.g. @Bestelmeyer2012].

We illustrate how a stochastic dynamic programming (SDP) algorithm
[@Mangel1988; @Marescot2013] can be driven by the
predictions from a Bayesian Non-Parametric (BNP) model of population dynamics [@Munch2005a].
This provides two distinct advantages compared with contemporary
approaches.  First, using a BNP sidesteps the need for an accurate
model-based description of the system dynamics.  Second, the BNP can
better reflect uncertainty that arises when extrapolating a model outside
of the data on which it was fit.  We illustrate that when the correct
model is not known, this latter feature is crucial to providing a robust
decision-theoretic approach in face of substantial structural uncertainty.



This paper represents the first ecological application of the SDP
decision-making framework without an a priori model of the underlying
dynamics.  In contrast to parametric models which can only reflect
uncertainty in parameter estimates, the BNP approach provides a more
state-space dependent representation of uncertainty.  This permits
a much greater uncertainty far from the observed data than near the
observed data.  These features allow the Gaussian Process Dynamic Programming (GPDP) approach to find robust
management solutions in face of limited data and without knowledge of
the correct model structure.



<!--
-  _Note on "not magic"_: honest uncertainty + SDP
--> 

<!--
The idea that any approach can perform well without either having to
know the model or have particularly good data should immediately draw
suspicion.  The reader must bear in mind that the strength of our approach
comes not from black-box predictive power from such limited information,
but rather, by providing a more honest expression of uncertainty outside
the observed data without sacrificing the predictive capacity near the
observed data. By coupling this more accurate description of what is known
and unknown to the decision-making under uncertainty framework provided
by stochastic dynamic programming, we are able to obtain more robust
management policies than with common parametric modeling approaches.
-->


<!--move to  much later *Or do we need this at all?* 
- _Note on comparing models_ (via value function rather than by "fit"). 
-->

To illustrate GPDP, we consider the performance of the GPDP policy against
the policies derived under several alternative parametric models [@Reed1979; @Ludwig1982; @Mangel1988]. 
The nature of decision-making problems provides a compelling and
pragmatic way to compare models: Rather than compare models in terms
of best fit to data, we define model performance in the concrete terms
of the decision-maker's objectives. 


<!-- Although much argument can be made
over the 'correct' objective function, e.g. how to account for the
social value of fish left in the sea vs. the commercial value of fish
harvested; see @Halpern2013 for further discussion of this issue.  We can
always compare model performance across multiple potential objectives.
The important point is that a decision-maker does not necessarily need
a model that provides the best mechanistic understanding or the best
long-term outcome, but rather the one that idelivers the best management
performance over the period of interest.
-->



Approach and Methods
====================

We first describe the requirements of dynamic optimization for the management of human intervention in natural resource systems. After that we describe three parametric models for population dynamics and the GP description of population dynamics.


### Requirements of dynamic optimization

Dynamic optimization  requires characterizing the dynamics of a state variable (or variables), a control action, and a value function.
In this paper, we consider only a single state variable, so that the ideas can be described as simply as possible. That is, we focus on a class of one-dimensional models of population dynamics as a
'best-case' scenario for the parametric model performance relative to
the GP.   This is abest-case scenario for the parametric models because the underlying dynamics are always simuated
from one the parametric models in the set, whereas in reality we never
have the "true" model among our candidates.  By choosing one-dimensional
models with few parameters, we limit the chance that poor performance
will be due to our inability to estimate parameters well, something
that becomes a more severe problem for higher-dimensional parametric
models. Further, the parametric models we consider are those most commonly
used in modeling stock-recruitment dynamics or to model sudden transitions
between alternative stable states.

### Parametric models

<!-- 
- Statement of the models
--> 


We let $X_t$ denote the size (numbers or biomass) of the focal population at time $t$ and assume that in the absence of take its dynamics are



$$ X_{t+1}= Z_t f(X_t, h_t, p) $$

Where Z(t) is log-normally distributed process stochasticity [@Reed1979] and $p$ is a vector of parameters to be estimated from the data.   We describe three choices for $f(X_t,p)$ in the next section.In this simple model, the control action is a harvest or take, $h_t$, measured in the same units as $X$, at time
$t$.  Thus, in the presence of take, the population size on the right hand side of Eqn 1 is replaced by
$S_t=X_t-h_t$.


To construct the value function, we consider a return when $X_t=x$ and harvest $h_t=h$ denoted as the reward, $R(x,h)$. For example, if the return is simply the harvest at time $t$, then $R(x,h)=min(x,h)$. We assume that future harvests are discounted relative to current ones at a constant rate of discount $\delta$ and ask for the harvest policy that maximizes total discounted harvest between the current time $t$ and a final time $T$, that is we seek to maximize over choices of harvest $\mathbf{E}_{X_t+1} [ \sum_{t = 0}^{T}  R_t(X_t, h_t) \delta^t]$, where the state dynamics are given by Eqn 1 and $\mathbf{E}$ denotes the expectation over the process stochasticity in the future population state.


In order to find that policy, we introduce the value function$V_t(x_t)$ representing the total discounted catch from time $t$ onwards given that $X_t=x$.  This value function satisfies an equation of Stochastic Dynamic Programming  [SDP, @Mangel1988; @Clark2000; @Clark2009], sometimes called the Bellman-Hamilton-Jacobi equation [@Mangel2014],

\begin{equation}
V_t(X_t) = \max_{h_t} \lbrace R(h_t, X_t) + \delta \cdot \mathbf{\mathrm{E}}_{X_t+1} \left[ V_{t+1}( X_{t+1}) | X_t, h_t \right] \rbrace
\end{equation}

where expectation is taken over all possible values of the next state, $X_t{+1}$, and maximized over all possible choices of harvest, $h_t$.  That is, at time $t$, when population size is $x$ and harvest $h_t$ is applied, the immediate return is $R(h_t, X_t)$. In the classical case, the sole source of uncertainty is the process noise term, $Z$, and thus the expectation above is equivalent to taking expectations over $Z$. That is

$$ \mathbf{\mathrm{E}}_{X_t+1} \left[ V_{t+1}( X_{t+1}) | X_t, h_t \right] = \mathbf{\mathrm{E}}_{Z} \left[ V_{t+1}( Z f(X_t - h_t))  | X_t, h_t \right] $$

where the population size after the take is $X_t-h_t$, which is then translated into $X_{t+1}$ by Eqn 1. 

In a Bayesian decision framework, the parameters governing the dynamics are also uncertain, so the
expectation in (1) involves averaging over the posterior distribution for the parameters, as well. That
is,

$$\mathbf{\mathrm{E}}_{X_t+1} \left[ V_{t+1}( X_{t+1}) | X_t, h_t \right] = \mathbf{\mathrm{E}}_{\theta,\mathrm{data}} \{ \mathbf{\mathrm{E}}_{Z | \theta, \mathrm{data}} \left[ V_{t+1}( Z f(X_t - h_t))  | X_t, h_t \right] \}$$


In the nonparametric case, the function $f$ too is uncertain and the expectation for the next state includes
uncertainty in $f$ as well. That is

$$\mathbf{\mathrm{E}}_{X_t+1} \left[ V_{t+1}( X_{t+1}) | X_t, h_t \right] = \mathbf{\mathrm{E}}_{\theta,\mathrm{data}} \{ \mathbf{\mathrm{E}}_{f, Z | \theta, \mathrm{data}} \left[ V_{t+1}( Z f(X_t - h_t))  | X_t, h_t \right] \}$$



We consider the finite time problem with $T=$`r MaxT`, which we solve using value iteration algorithm [@Mangel1988; @Clark2000].



\subsubsection{Parametric models}\label{parametric-models}

We consider three candidate parametric models of the stock-recruitment
dynamics: The Ricker model, the Allen model [@Allen2005], the Myers
model [@Myers1995], equations (@ricker)-(@myers), which we parameterize as follows for $f(S_t,p)$ in Eqn 1. In all three, we let $K$ denote the carrying capacity and $r$ the maximum per capita growth rate. In the Ricker model,


(@ricker) $$ f(S(t)|p) = S_t e^{r \left(1 - \frac{S_t}{K} \right) } $$

In the Allen model,

(@allen) $$ f(S(t)|p) = S_t e^{r \left(1 - \frac{S_t}{K}\right)\left(S_t - X_c\right)} $$

Where $r$ and $K$ are as before, and $X_c$ denotes the location of the unstable steady state (i.e., the tipping point).  

In the Myers model, 

(@myers) $$ f(S(t) | p)  = \frac{r S_t^{\theta}}{1 + \frac{S_t^\theta}{K}}$$

Where $\theta = 1$ corresponding to Beverton-Holt dynamics and $\theta$ > 2 leads to Allee effects and multiple stable states. 

The Ricker model involves two parameters, corresponding
to a growth rate and a carrying capacity, and cannot support alternative
stable growth rate dynamics. The Allen model resembles the Ricker
dynamics with an added Allee effect parameter [@Courchamp2008], below
which the population cannot persist.  The Myers model also has three
parameters and contains an Allee threshold, but has compensatory rather
than over-compensatory density dependence (resembling a Beverton-Holt
curve rather than a Ricker curve at high densities.)  

The multiplicative log-normal stochasticity perturbs the growth predicted 
by each of these deterministic model skeletons. This 
introduces one additional parameter $\sigma$ that must be estimated. 

Because of our interest in management performance in the presence of
tipping points, all of our simulations are based on the Allen model.
The Allen model is thus "structurally correct" and is expected to provide
a best-case scenario when the 'true' dynamics are known.
The Ricker model is a reasonable approximation of these dynamics far from the Allee
threshold (but lacks threshold dynamics), while the Myers model shares
the essential feature of a threshold but differs in the structure. 

We consider a period of `r Tobs` years of training data: long
enough that the estimates do not depend on the particular realization,
while longer times are not likely to provide substantial improvement
(i.e. the results are not sensitive to this interval).  Each of the models
(described below) is fit to the same training data (Figure 1).


<!-- Bayesian inference of parametric models --> 

We inferred posterior distributions for the parameters of each model by Gibbs sampling (@Gelman2003 implemented in R
[@RTeam] using jags, [@R2jags]).  We choose uniform priors
for all parameters (See appendix Tables S1-S3; R code provided). We
show one-step-ahead predictions of these model fits in Figure 1.
We tested each chain for Gelman-Rubin convergence and results were
robust to longer runs.  For each simulation we also applied several
commonly used model selection criteria (AIC, BIC, DIC, see @Burnham2002)
to identify the best fitting model.


### The Gaussian Process model

The core difference for our purpose between the GP representation and the parametric
models we have already introduced is the "non-parametric" property, which
means that an estimated GP model is defined explicitly in reference to 
some observed data.  Unlike the models above, it cannot be specified by
the value of some parameters alone.  The nonparametric moniker for the
GP approach is unfortunate as (a) it still involves the estimation of
parameters, and (b), statisticians also use non-parametric to mean models
that make only weak distributional assumptions  [e.g. @Lehmann1975].
However, we retain the terminology for consistency with previous studies.


<!-- Statement of model -->  

The use GP methods to formulate a predictive
model is relatively new in the context of modeling dynamical systems
[@Kocijan2005], and was first introduced in the context ecological
modeling and fisheries management in @Munch2005.  GP models have
subsequently been used to test for the presence of Allee effects
[@Sugeno2013a], estimate the maximum reproductive rate [@Sugeno2013b],
determine temporal variation in food availability [@Sigourney2012], and
provide a basis for identifying model-misspecification [@Thorson2014].
An accessible and thorough introduction to the formulation and use of
GPs can be found in @Rasmussen2006. 

A GP is an infinite dimensional (because it is a function of a continuous input
variable) stochastic process for which any realization is a multivariate
normal distribution. Thus, to characterize the GP we need a mean function
and a covariance matrix. We proceed as follows.


Assuming the data $X_o$ are observed with some process noise,

$$X_{t+1} = f(X_t) + \varepsilon$$

$\varepsilon$ IID normal, variance $\sigma_n^2$. Note that we have chosen to
assume additive noise.  While we could just as easily consider log-normal noise
as in the parametric models, we make this choice to bring home the point that the
Gaussian process approach need not have structurally correct noise form either.  

Then under the GP, given a vector of observed data $X_0$ the predicted data $X_p$ obeys

$$f(X_p|X_o) \sim \mathcal{N}(E,C)$$
$$E = K(X_p, X_o) \left(K(X_o,X_o) - \sigma \mathbb{I} \right)  ^{-1} y$$
$$C = K(X_p, X_p) - K(X_p, X_o) K(X_o,X_o)^{-1} K(X_o, X_p)$$

Where $K$ is the Gaussian process kernel.  (Compare to  Eqn 2.22 of @Rasmussen2006 for a more detailed derivation.) Throughout, we use the radial basis function kernel:

$$ K(x,y) = \exp\left(\frac{-(x-y)^2}{2 \ell^2} \right)$$

We use vague inverse Gamma priors on both the lengthscale $\ell$ and process noise $\sigma$ parameters, of the form

$$f(x; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha - 1}\exp\left(-\frac{\beta}{x}\right)$$

For the $\sigma$ prior, $\alpha = $ `r s2.p[1]` and $\beta = $ `r s2.p[2]`.  For $\ell$ prior,  $\alpha = $ `r d.p[1]` and $\beta = $ `r d.p[2]`.  



Using the simulated training data we also estimate a Gaussian process
defined by a radial basis function kernel of two parameters: $\ell$,
which gives the characteristic length-scale over which correlation between
two points in state-space decays, and $\sigma$, which gives the scale
of the process noise by which observations $Y_{t+1}$ may differ from
their predicted values $X_{t+1}$ given an observation of the previous
state, $X_t$. @Munch2005a gives an accessible introduction to the use
of Gaussian processes in providing a Bayesian nonparametric description
of the stock-recruitment relationship.

<!-- Inference of the BNP model --> 


We use a Metropolis-Hastings Markov Chain Monte Carlo (@Gelman2003) to
infer posterior distributions of the two parameters of the GP (Figure
S13, code in appendix), under weakly informative Gaussian priors
(see parameters in table S5). As the posterior distributions differ
substantially from the priors (Figure S13), we can be assured that most
of the information in the posterior comes from the data rather than the
prior belief.

<!-- SDP via the model  FIXME much more detail here--> 

The method of Gaussian Process Dynamic Programming
--------------------------------------------------

It is reasonably straight-forward to derive the harvest policy from the
estimated BP by inserting it into a SDP algorithm. The only difference 
is that the uncertainty in the future state under the GP, $X_{t+1}$,
includes both process uncertainty (based on the estimation of $\sigma$) 
and "structural uncertainty" of the posterior collection of curves.  
(Recall that given the parameters, a parametric model is represented
by a single curve, while the GP remains a distribution of curves).  
From the GP posteriors, we can write down the (discritized) transition
matrix representing the probability of going to each state $X_{t+1}$ 
given any current state $X_t$ and any harvest $h_t$ (See the function
`gp_transition_matrix()` in the provided R package). Given this transition
matrix, we use the same value iteration algorithm as in the parametric
case to determine the optimal policy.   



Results
=======


Parametric and GP models for population dynamics
------------------------------------------------

To ensure our results are robust to our choice of parameters, 
we perform our analysis over 96 different scenarios. To better
understand the process, we will first describe in detail the
results of a single scenario.  

<!-- 
### Figure 1: Fitted Models

- All models fit the data quite well
- Information criteria would pick the simple, incorrect model.
--> 


```{r figure_1, dependson=c("assemble-models", "par-fns", "plot-options"), fig.cap="Points show the training data of stock-size over time.  Curves show the posterior step-ahead predictions based on each of the estimated models. Observe that all models are fitting the data reasonably well.", fig.width=8, fig.height=6}
step_ahead_posteriors <- function(x){
  gp_f_at_obs <- gp_predict(gp, x, burnin=1e4, thin=300)
  df_post <- melt(lapply(sample(100, 30),  
  function(i){
    data.frame(time = 1:length(x), stock = x, 
                GP = mvrnorm(1, gp_f_at_obs$Ef_posterior[,i], gp_f_at_obs$Cf_posterior[[i]]),
                True = step_ahead(x,f,p),  
                MLE = step_ahead(x,f,est$p), 
                Allen = step_ahead(x, allen_f, pardist[i,]), 
                Ricker = step_ahead(x, ricker_f, ricker_pardist[i,]), 
                Myers = step_ahead(x, myers_f, myers_pardist[i,]))
  }), id=c("time", "stock"))
}
df_post <- step_ahead_posteriors(x)
figure1b_posteriors <- ggplot(df_post) + geom_point(aes(time, stock)) + 
  geom_line(aes(time, value, col=variable, group=interaction(L1,variable)), alpha=.1) + 
  facet_wrap(~variable) + 
  scale_colour_manual(values=colorkey, guide = guide_legend(override.aes = list(alpha = 1))) +  
  theme(legend.position="none")
figure1b_posteriors



write.csv(df_post, "components/data/figure1.csv")
```

All models fit the observed data rather closely and with relatively small uncertainty, as illustrated in the posterior predictive curves in Figure 1.  Figure 1 shows the training data of stock sizes observed over time as points, overlaid with the step-ahead predictions of each estimated model using the parameters sampled from their posterior distributions.  Each model manages to fit the observed data rather closely. Compared to the true model most estimates appear to over-fit, predicting fluctuations that are actually due purely to stochasticity.  All model selection criteria shown in Table 1 penalize more complex models and show a preference for the simpler Ricker model over the more complicated alternate stable state models (Allen and Myers).  Details on MCMC estimates for each model, traces, and posterior distributions can be found in the appendix.   

```{r deviances, dependson=c("dic_calc"), include=FALSE, echo=FALSE}
allen_deviance  <- - posterior.mode(pardist[,'deviance'])
ricker_deviance <- - posterior.mode(ricker_pardist[,'deviance'])
myers_deviance  <- - posterior.mode(myers_pardist[,'deviance'])
true_deviance   <- 2*estf(c(p, sigma_g))
mle_deviance    <- 2*estf(c(est$p, est$sigma_g))
aictable <- data.frame(Allen = allen_deviance + 2*(1+length(bayes_pars)),  # +1 for noise parameter
                       Ricker = ricker_deviance + 2*(1+length(ricker_bayes_pars)),
                       Myers = myers_deviance + 2*(1+length(myers_bayes_pars)), 
                       row.names = c("AIC"))
bictable <- data.frame(Allen = allen_deviance + log(length(x))*(1+length(bayes_pars)), 
                       Ricker = ricker_deviance + log(length(x))*(1+length(ricker_bayes_pars)),
                       Myers = myers_deviance + log(length(x))*(1+length(myers_bayes_pars)), 
                       row.names = c("BIC"))
xtable::xtable(rbind(dictable, aictable, bictable))
```

```{r Table1, dependson="deviances", include=TRUE, results="asis", echo=FALSE, cache=FALSE}
xtable::xtable(rbind(dictable, aictable, bictable), caption="Model selection scores for several common criteria all (wrongly) select the simplest model. As the true (Allen) model is not distinguishable from the simpler (Ricker) model in the region of the observed data, this error cannot be avoided regardless of the model choice criterion. This highlights the danger of model choice when the selected model will be used outside of the observed range of the data.")
```


<!-- 
### Figure 2 
- Data comes from limited region of state-space 
- (Should really show uncertainty of all models here.  Capture the forecast uncertainty several steps down the road?)   
-->

```{r figure_2, dependson=c("assemble-models", "plot-options"), fig.cap="Graph of the inferred Gaussian process compared to the true process and maximum-likelihood estimated process.  Graph shows the expected value for the function $f$ under each model.  Two standard deviations from the estimated Gaussian process covariance with (light grey) and without (darker grey) measurement error are also shown.  The training data is also shown as black points.  The GP is conditioned on (0,0), shown as a pseudo-data point.", fig.width=8, fig.height=6}
x_grid_short <- x_grid[1:40]
gp_short <- gp_predict(gp, x_grid_short, burnin=1e4, thin=300)
models_posteriors <- 
  melt(lapply(sample(100, 50), 
              function(i){
    sample_gp <- mvrnorm(1, 
                            gp_short$Ef_posterior[,i],         
                            gp_short$Cf_posterior[[i]])
    data.frame(stock = x_grid_short, 
               GP = sample_gp,
               y = sample_gp,
               ymin = sample_gp - 2 * sqrt(gp_short$E_Vf), 
               ymax = sample_gp + 2 * sqrt(gp_short$E_Vf), 
               True = sapply(x_grid_short,f,0, p),  
               MLE = sapply(x_grid_short,f,0, est$p), 
               Allen = sapply(x_grid_short, allen_f, 0, pardist[i,]), 
               Ricker = sapply(x_grid_short, ricker_f, 0, ricker_pardist[i,]), 
               Myers = sapply(x_grid_short, myers_f, 0, myers_pardist[i,]))
             }), 
       id=c("stock", "y", "ymin", "ymax"))
ggplot(models_posteriors) + 
    geom_ribbon(aes(x=stock, y=y, ymin=ymin, ymax=ymax, group=L1), 
                  fill = "gray80", 
                  data=subset(models_posteriors, variable == "GP")) + 
    geom_line(aes(stock, value, col = variable, 
                  group=interaction(L1,variable)), 
              alpha=.2) + 
    geom_point(data = obs, aes(x,y), alpha = 0.8) + 
    xlab(expression(X[t])) + ylab(expression(X[t+1])) +
    facet_wrap(~variable) + 
    scale_colour_manual(values=colorkey) +  
    theme(legend.position="none")

write.csv(models_posteriors, "components/data/figure2.csv")
write.csv(obs, "components/data/obs.csv")
```

The mean inferred state space dynamics of each model relative to the
true model used to generate the data is shown in Figure 2, predicting
the relationship between observed stock size (x-axis) to the stock
size after recruitment the following year.  Note that in contrast to
the  other models shown, the expected Gaussian process corresponds to a
distribution of curves - as indicated by the gray band - which itself
has a mean shown in black. Parameter uncertainty (not shown) spreads
out the estimates further.  The observed data from which each model is
estimated is also shown.  The observations come from only a limited region
of state space corresponding to unharvested or weakly harvested system.
No observations occur at the theoretical optimum harvest rate or near
the tipping point.



```{r figure_3, fig.width=8, fig.height=6, fig.cap="Outside the range of the training data (Figure 1), the true dynamics (black dots) fall outside the uncertainty (two standard deviations, colored bands) of the structurally incorrect parametric models (Ricker, Myers), but inside the uncertainty predicted by the GP. Points show the stock size simulated by the true model.  Overlay shows the range of states predicted by each model, based on the state observed in the previous time step. The Ricker model always (wrongly) predicts positive population growth, while the actual population shrinks in each step as the initial condition falls below the Allee threshold of the underlying model (Allen).  Note that because it does not assume a parametric form but instead relies more directly on the data, the GP is both more pessimistic and more uncertain about the future state than the parametric models.", dependson="plot-options"}
y <- numeric(8)
y[1] <- 4.5
for(t in 1:(length(y)-1))
      y[t+1] = z_g() * f(y[t], h=0, p=p)
# predicts means, does not reflect uncertainty estimate!
crash_data <- step_ahead_posteriors(y)
crash_data <- subset(crash_data, variable %in% c("GP", "Allen", "Ricker", "Myers"))
ggplot(crash_data) + 
  geom_boxplot(aes(as.factor(as.integer(time)), value, 
                   fill = variable, col=variable), 
               alpha=.7, outlier.size=1, position="identity") + 
#  geom_line(aes(time, value, col = variable, 
#            group=interaction(L1,variable)), alpha=.1) + 
  geom_point(aes(time, stock), size = 3) + 
  scale_fill_manual(values=colorkey[c("GP", "Allen", "Ricker", "Myers")], 
                      guide = guide_legend(override.aes = list(alpha = 1))) +  
  scale_colour_manual(values=colorkey[c("GP", "Allen", "Ricker", "Myers")], 
                      guide = guide_legend(override.aes = list(alpha = 1))) +  
  facet_wrap(~variable) + 
  theme(legend.position="none") + xlab("time") + ylab("stock size") 

write.csv(crash_data, "components/data/figure3.csv")
```




<!--
### Figure 4: Inferred Policies

- Inferred policies differ substantially among models
- The structurally correct model and the GP are close to the true model
- alternatives are not close
--> 


```{r figure_4, dependson = c("assemble-opt"), fig.cap="The steady-state optimal policy (infinite boundary) calculated under each model.  Policies are shown in terms of target escapement, $S_t$, as under models such as this a constant escapement policy is expected to be optimal [@Reed1979]."}
policies <- melt(data.frame(stock=x_grid, sapply(OPT, function(x) x_grid[x])), id="stock")
names(policies) <- c("stock", "method", "value")

ggplot(policies, aes(stock, stock - value, color=method)) +
  geom_line(lwd=1.2, alpha=0.8) + xlab("stock size") + ylab("escapement")  +
  scale_colour_manual(values=colorkey)

write.csv(policies, "components/data/figure4.csv")
```

Despite the similarities in model fits to the observed data, the
policies inferred under each model differ widely, as shown in Figure 4.
Policies are shown in terms of target escapement, $S_t$.  Under models
such as this a constant escapement policy is expected to be optimal
[@Reed1979], whereby population levels below a certain size $S$ are
unharvested, while above that size the harvest strategy aims to return the
population to $S$, resulting in the hockey-stick shaped policies shown.
Only the structurally correct model (Allen model) and the GP produce
policies close to the true optimum policy (where both the underlying
model structure and parameter values are known without error).


```{r figure_5, dependson=c("sim"), fig.cap="In the management context, GPDP outperforms approaches based on parametric models. Shown are 100 replicate simulations of the stock dynamics (eq 1) under the policies derived from each of the estimated models, as well as the policy based on the exact underlying model.", fig.width=6, fig.height=6}
ggplot(sims_data) + 
  geom_line(aes(time, fishstock, group=interaction(reps,method), color=method), alpha=.1) +
  scale_colour_manual(values=colorkey, guide = guide_legend(override.aes = list(alpha = 1))) + 
  facet_wrap(~method) + guides(legend.position="none")

write.csv(sims_data, "components/data/figure5.csv")
```

The consequences of managing 100 replicate realizations of the simulated
fishery under each of the policies estimated is shown in Figure 5.
As expected from the policy curves, the structurally correct model
under-harvests, leaving the stock to vary around it's un-fished optimum.
The structurally incorrect Ricker model over-harvests the population
passed the tipping point consistently, resulting in the immediate crash
of the stock and thus derives minimal long-term catch.

The results across replicate stochastic simulations can most
easily be compared by using the relative differences in net present
value realized by each of the model. Figure 6 shows the distribution of values
realized by each simulation shown in Figure 5.  The GPDP
most consistently realizes a value close to the optimal solution,
and importantly avoids ever driving the system across the tipping point,
which results in the near-zero value cases in the parametric models.


```{r figure_6, dependson=c("profits"), fig.cap="Histograms of the realized net present value of the fishery over a range of simulated data and resulting parameter estimates. For each data set, the three models are estimated as described above. Values plotted are the averages of a given policy over 100 replicate simulations. Details and code provided in the supplement.", fig.width=6, fig.height=6}

ggplot(actual_over_optimal, aes(value)) + geom_histogram(aes(fill=variable)) + 
  facet_wrap(~variable, ncol=2)  + 
  guides(legend.position = "none") +
  xlab("Total profit by replicate") + 
  scale_fill_manual(values=colorkey) # density plots fail when delta fn

# ggplot(actual_over_optimal, aes(value)) + geom_histogram(aes(fill=variable), binwidth=0.1) + 
#  xlab("Total profit by replicate")+ scale_fill_manual(values=colorkey)
# ggplot(actual_over_optimal, aes(value, fill=variable, color=variable)) + # density plots fail when delta fn
#  stat_density(aes(y=..density..), position="stack", adjust=3, alpha=.9) + 
#  xlab("Total profit by replicate")+ scale_fill_manual(values=colorkey)+ scale_color_manual(values=colorkey)

write.csv(actual_over_optimal, "components/data/figure6.csv")
```

Sensitivity Analysis
--------------------


These results are not sensitive to the modeling details of the simulation.
The GPDP estimate remains very close to the optimal solution obtained
by knowing the true model across changes to the training simulation,
noise scale, parameters or structure of the underlying model, as seen in
Figure 7. The sensitivity analysis uses a factorial design of eight replicates 
of three different randomly generated parameter sets for each of two different 
generating models (Allen and Myers) over two different noise levels 
(0.01 and 0.05), for a total of 8 x 3 x 2 x 2 = 96 scenarios. In Figure 7, 
we pool results across different random seeds (111, 222,
333), noise values of 0.01 and 0.05 (not large enough to violate the
self-sustaining criterion of @Reed1979), and different randomly generated
parameters sets for each model, and using either the Myers or Allen as the
underlying structure.  While the GPDP performs slightly closer to optimal
on the smaller noise levels, and closer on the Allen model then the Myers
model, the differences are small compared to the values observed in using
the wrong parametric model.  The GPDP solution consistently avoids either
failing to harvest at all or crashing the stock, which results in the very
low values seen in parametric models in the previous figure. The
sensitivity analysis is compared against the optimal solution alone and
not shown against the estimates of the various parametric models (but
see Figure 6), where convergence of the Gibbs sampling could contribute
to their poorer performance if not checked by hand for each configuration.



Changing the noise or the distance between stable and unstable points
does not impact the performance of the GP relative to the optimal solution
given the true model and true parameters.  The parametric models are
more senstive to this difference.  Large noise relative to the distance between the stable and unstable
point increases the chance of a stochastic transition.  More precisely,
if we let $L = K - x_c$, then the probability of a transition in a given
window of time $T$ scales as 

$$P(x_t < x_c | t \in T) \propto  \exp\left(\frac{L^2}{\sigma^2}\right),$$


(following the Ahrenius relationship, see @Gardiner2009 for the derivation).
As a result, the impact of using a model which underestimates the risk of harvesting
past the critical point is much more severe, since this such a situation
occurs more often.  Conversely, with large enough distance between the
optimal escapement and unstable points relative to the noise, the chance
of a transition becomes vanishingly small and all models can be modeled
near-optimally. Models that underestimate the cost incurred by stock
sizes fluctuating significantly below the optimal escapement level will
not perform poorly as long as those fluctuations are sufficently rare.  Figure 7
shows that the GPDP is little effected by increasing noise or increasing Allee
effects over much of the range.  

<!--FIXME the above para is not particularly convincing. Quantitative please? -->


```{r figure_7, fig.width=6, dependson=c("sensitivity-trends"), fig.cap="The effect of increasing noise or decreasing Allee threshold levels on the net present value of the fishery when managed under the GPDP, relative to managing under the true model (with known parameters).  Other than the focal parameter (noise, Allee threshold), other parameters are held fixed as above to illustrate this effect."}

plot_sigmas <- ggplot(vary_sigma, aes(noise, value)) + 
  geom_point() + ylab("Net present value") + 
  xlab("Level of growth stochasticity") + theme_bw() 
plot_allees <- ggplot(vary_allee, aes(allee, value)) + 
  geom_point()+ ylab("Net present value") + 
  xlab("Allee threshold stock size") + theme_bw()
cboettigR::multiplot(plot_sigmas, plot_allees, cols=2) 

write.csv(vary_sigma, "components/data/vary_sigma.csv")
write.csv(vary_allee, "components/data/vary_sigma.csv")
```



Discussion 
==========


Though simple mechanistically motivated models offer the greatest
potential to increase our basic understanding of ecological processes
[@Cuddington2013; @Geritz2012], such models can be both inaccurate and
misleading when relied upon in a quantitative decision making framework.
In this paper we have tackled two aspects of uncertainty that are both
common to many ecological decision-making problems and fundamentally
challenging to existing approaches which largely rely on parametric
models:

1. We do not know what the correct models are for ecological systems.
1. We have limited data from which to estimate the model -- in particular,
   such models may be misleading in predicting the probability of outcomes
   outside the training data.  

We have illustrated how the use of non-parametric approaches can provide
more reliable solutions in the sequential decision-making problem. 

### Traditional model-choice approaches can be positively misleading.  

These results illustrate that model-choice approaches would be positively
misleading -- supporting simpler models that cannot express tipping point
dynamics merely on account of them being similar.  As the data shown
comes only from the basin of attraction near the unfished equilibrium,
near which all of the models are approximately linear and approximately
identical. 

Model selection criteria trade off model complexity and fit to the data. 
When the data come from a limited region of state-space -- as is necessarily 
the case whenever there is a potential concern about tipping point dynamics --
simpler models therefore fit just as well and tend to be selected more foten than complex 
ones.  This approach would be appropriate when the dynamics can be expected 
to remain in the region of the training data; for instance, if we only 
considered the forecasting accuracy of the unfished population dynamics under
each model.  

In contrast, the decision-maker's problem of setting appropriate harvest levels
cannot exclude regions of state-space outside the observed range when integrating
over all possible decisions to find the optimal choice.  Such problems are not 
constrained to fisheries management but ubiquitous across ecological decision-making
and conservation where the greatest concerns involve entering previously unobserved
regions of state-space -- whether that is the collapse of a fishery, the spread
of an invasive, or the loss of habitat.  

### GPDP population dynamcis capture larger uncertainty in regions where the data are poor 

<!-- FIXME More emphasis/explanation on _how_ it does this --> 

The parametric models perform worst when they propose a management strategy
outside the range of the observed data. The non-parametric Bayesian approach, 
in contrast, allows a predictive model that expresses a great deal of uncertainty
about the probable dynamics outside the observed range, while retaining very
good predictive accuracy in the range observed.  The management policy 
dictated by the GP balance this uncertainty against
the immediate value of the harvest, and act to stabilize the population 
dynamics in a region of state space in which the predictions can be 
reliably reflected by the data.  


<!--

### Risk-prone and risk-adverse value functions

The degree to which the decision-making part of the algorithm (the SDP)
chooses to explore or avoid the resulting region of uncertainty can also
be influenced by the curvature of the value (profit) function $R$. Both
to simplify the intuition and avoid biasing this result, we have chosen
profits that are linear in the catch and thus neither risk-prone nor
risk adverse.  Making this function concave, representing the typical
assumption of diminishing returns, would make the SDP more risk-adverse
(as larger-than-expected stock sizes offer diminished returns relative
to the cost of smaller-than-expected stock sizes), and strengthen the
result shown here in which the BNP solution tends to avoid the region
of uncertainty.  Sufficiently convex or risk-prone functions could lead
the SDP to attempt higher exploitation rates despite the uncertainty.
Understanding the relative roles of such functions would be a promising
direction for future investigation.

-->


### The role of the prior

Lastly, it should be noted that outside the data, the GP reverts to
the prior, and consequently the choice of the prior can also play a
significant role in determining the optimal policy inferred by the
SDP.  In the examples shown here we have selected a prior that is both
relatively uninformative (due to the broad priors placed on its parameters
$\ell$ and $\sigma$ and simple (mean zero, radial basis function kernel).
In practice, both the choice of mean and the choice of the covariance
function may be chosen to confer particular biological properties,
as well as more biologically informed priors for $\ell$ and $\sigma$.
In principle, this may allow a manager to improve the performance of the
GPDP by adding only enough additional detail as is justified.
For instance, it would be possible to use a linear or a Ricker-shaped
mean in the prior without making the much stronger assumption that the
Ricker is the structurally correct model [@Sugeno2013a]. Future research
should focus on identifying criteria to ensure the prior and the value function
are chosen appropriately for the problem at hand.


<!--

Future directions
-----------------


In this simulated example, the underlying dynamics are truly governed
by a simple parametric model, allowing the parametric approaches to be
more accurate.  Similarly, because the dynamics are  one-dimensional
dynamics and lead to  stable nodes (rather than other attractors such
as limit-cycles resulting in oscillations), the training data provides
relatively limited information about the dynamics.  For these reasons,
we anticipate that in higher-dimensional examples characteristic of
ecosystem management problems that the GP approach will
prove even more valuable.

In our treatment here we have ignored the possibility of learning during
the management phase, in which the additional observations of the stock
size could potentially improve parameter estimates.  Of particular
interest in the context of the extreme uncertainty considered here
is the notion of "active adaptive management" or "adaptive probing",
which may actively seek to reduce uncertainty.  The concept of adaptive
probing is one area that has explicitly addressed the extrapolation
uncertainty addressed here, though typically without the additional
issue of model uncertainty.  As a result, adaptive probing strategies
suggest rather opposite conclusions than what we observe here. Such
adaptive probing or Dual Control (e.g. @Ludwig1982) approaches trade
off short term utility by choosing actions that can reduce uncertainty.
Adaptive probing strategies arise when it is valuable to intentionally
force a system far from the observed values even when the expected value
such actions is low, as it provides much faster learning and consequent
reduction of model uncertainty that can allow greater value to be derived
later on.  For instance, @Ludwig1982 show that it may be advantageous
to fish an unexploited population very heavily at first to obtain a
better estimate of the recruitment rate.  This intuitive strategy when a
population is governed by a Ricker or Beverton-Holt-like dynamic would
clearly be disastrous if instead the dynamics contained an unforeseen
tipping point.  The best way to learn where the edge lies may be to
walk up to it, but it is also the most dangerous.  Future work should
attempt to understand when such active adaptive learning is valuable,
and when it will increase the risk of an irreversible transition.

-->

Acknowledgments
===============

This work was partially supported by NOAA-IAM grant to SM and Alec McCall and administered
through the Center for Stock Assessment Research, a partnership between
the University of California Santa Cruz and the Fisheries Ecology
Division, Southwest Fisheries Science Center, Santa Cruz, CA and by NSF
grant EF-0924195 to MM and NSF grant DBI-1306697 to CB.

\appendix
\renewcommand*{\thefigure}{S\arabic{figure}}
\renewcommand*{\thetable}{S\arabic{table}}
\setcounter{figure}{0}}
\setcounter{table}{0}}

Appendix: Model definitions and estimation
===========================================



```{r figure_S1, fig.cap = "Traces from the MCMC estimates of the GP model show reasonable mixing (no trend) and sampling rejection rate (no piecewise jumps)", dependson="plot-options"}
gp_assessment_plots$traces_plot
```


```{r figure_S2, fig.cap="Posterior distributions from the MCMC estimate of the GP model. Prior curves shown in red."}
gp_assessment_plots$posteriors_plot
```


\newpage



Training data (Figures 1-6)
----------------------------

Each of our models $f(S_t)$ must be estimated from training data, which
we simulate from the Allen model with parameters $r = $ `r p[1]`, 
$K =$ `r p[2]`, $C =$ `r p[3]`, and  $\sigma_g =$ `r sigma_g` 
for $T=$ `r Tobs` timesteps, starting at initial condition $X_0 = $ `r Xo`. 
The training data can be seen in Figure 1.  

Training data for sensitivity analyses
-------------------------------------

A further 96 unique training data sets are generated for the sensitivity analysis, as described in the main text.  



Code
----

All code used in producing this analysis can be found in the R package accompanying the paper, available from [http://github.com/cboettig/nonparametric-bayes]. The code for all results shown here is dynamically embedded into the manuscript using `knitr` [@knitr].  


<!--
Data
----

While the data can be regenerated using the code provided, for convenience CSV files of the data shown in each graph are also provided, along with appropriate metadata written in the Ecological Metadata Language (EML).  

-->



```{r figure_S3, fig.height=6, fig.cap="Traces from the MCMC estimates of the Ricker model show reasonable mixing (no trend) and sampling rejection rate (no piecewise jumps)", fig.width=6}
plot_ricker_traces
```

```{r figure_S4, fig.cap="Posteriors from the MCMC estimate of the Ricker model", fig.width=6, fig.height=4}
ggplot(ricker_posteriors, aes(value)) + 
  stat_density(geom="path", position="identity", alpha=0.7) +
  facet_wrap(~ variable, scale="free", ncol=2)
write.csv(ricker_posteriors, "components/data/ricker_posteriors.csv")
```


```{r Table S1, results = "asis"}
pander::pandoc.table(ricker_priors_xtable,
  caption = "Parameterization range for the uniform priors in the Ricker model")
```


```{r figure_S5, fig.height=6, fig.cap="Traces from the MCMC estimates of the Myers model show reasonable mixing (no trend) and sampling rejection rate (no piecewise jumps)", fig.width=6}
plot_myers_traces
```

```{r figure_S6, fig.cap="Posterior distributions from the MCMC estimates of the Myers model", fig.width=6, fig.height=6}
ggplot(myers_posteriors, aes(value)) + 
  stat_density(geom="path", position="identity", alpha=0.7) +
  facet_wrap(~ variable, scale="free", ncol=2)
write.csv(myers_posteriors, "components/data/myers_posteriors.csv")
```


```{r TableS2, results="asis"}
pander::pandoc.table(myers_priors_xtable,
           caption = "Parameterization range for the uniform priors in the Myers model")
```


```{r figure_S7, fig.height=6, fig.cap="Traces from the MCMC estimates of the Allen model show reasonable mixing (no trend) and sampling rejection rate (no piecewise jumps)", fig.width=6}
plot_allen_traces
```

```{r figure_S8, fig.cap="Posteriors from the MCMC estimate of the Allen model", fig.width=6, fig.height=6}
ggplot(allen_posteriors, aes(value)) + 
  stat_density(geom="path", position="identity", alpha=0.7) +
  facet_wrap(~ variable, scale="free", ncol=2)
write.csv(allen_posteriors, "components/data/allen_posteriors.csv")
```


```{r TableS3, results = "asis"}
pander::pandoc.table(allen_priors_xtable,
  caption = "Parameterization range for the uniform priors in the Allen model")
```



```{r sensitivity-calc}
source("components/sensitivity.R")

models <- c("Myers","Allen")

parameters <- list(
  Myers = list(
    c(r=1.5 + rnorm(1, 0, 1), theta=2.5 + rnorm(1, 0, 1), K=10 + rnorm(1, 0, 1)),
    c(r=1.5 + rnorm(1, 0, 1), theta=2.5 + rnorm(1, 0, 1), K=10 + rnorm(1, 0, 1))),
  Allen = list(
    c(r=2 + rnorm(1, 0, 1), K=10 + rnorm(1, 0, 1), C=4 + rnorm(1, 0, 1)),
    c(r=2 + rnorm(1, 0, 1), K=10 + rnorm(1, 0, 1), C=4 + rnorm(1, 0, 1)))
)
nuisance_pars <- c("sigma_g")
nuisance_values <- list(sigma_g = c(0.01, 0.05))

model <- "Allen"
allen1.01 <- sensitivity(model, 
                   parameters = parameters[[model]][[1]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[1]), 
                   seed=c(1111, 2222, 3333))
allen2.01 <- sensitivity(model, 
                   parameters = parameters[[model]][[2]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[1]), 
                   seed=c(1111, 2222, 3333))
allen1.05 <- sensitivity(model, 
                   parameters = parameters[[model]][[1]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[2]), 
                   seed=c(1111, 2222, 3333))
allen2.05 <- sensitivity(model, 
                   parameters = parameters[[model]][[2]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[2]), 
                   seed=c(1111, 2222, 3333))

model <- "Myers"
Myers1.01 <- sensitivity(model, 
                   parameters = parameters[[model]][[1]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[1]), 
                   seed=c(1111, 2222, 3333))
Myers2.01 <- sensitivity(model, 
                   parameters = parameters[[model]][[2]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[1]), 
                   seed=c(1111, 2222, 3333))
Myers1.05 <- sensitivity(model, 
                   parameters = parameters[[model]][[1]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[2]), 
                   seed=c(1111, 2222, 3333))
Myers2.05 <- sensitivity(model, 
                   parameters = parameters[[model]][[2]], 
                   nuisance = c(sigma_g = nuisance_values$sigma_g[2]), 
                   seed=c(1111, 2222, 3333))

## Assemble into data.frame
allen_dat <- rbind(allen1.01, allen1.05, allen2.01, allen2.05) 
m <- rbind(Myers1.01, Myers1.05, Myers2.01, Myers2.05)
myers_dat <- m[c(1:2,4,3,5:8)]
names(myers_dat) <- names(allen_dat)
model_dat <- rbind(allen_dat, myers_dat)
dat <- model_dat
dat$pars.r <- factor(dat$pars.r, labels=c("A", "B", "C", "D"))
dat <- dat[c(1:2,5:6, 8, 7)]
dat$noise <- factor(dat$noise)
names(dat) <- c("model", "parameters", "replicate", "simulation", "noise", "value")

## Extract acutal parameter values corresponding to each parameter set
p1 = levels(factor(model_dat$pars.r))
p2 = levels(factor(model_dat$pars.K))
p3 = levels(factor(model_dat$pars.C))

setA = as.numeric(c(r = p1[1], K = p2[1], theta = p3[1]))
setB = as.numeric(c(r = p1[2], K = p2[2], theta = p3[2])
setC = as.numeric(c(r = p1[3], K = p2[3], C = p3[3]))
setD = as.numeric(c(r = p1[4], K = p2[4], C = p3[4]))
AllenParameterSets <- rbind(setA, setB)
MyersParameterSets <- rbind(setC,setD)

sensitivity_dat <- dat
```

```{r figure_S9, fig.height=6, fig.width=10, dependson=c("sensitivity-calc", "export-data"), fig.cap="Sensitivity Analysis.  Histograms shows the ratio of the realized net present value derived when managing under the GPDP over the optimal value given the true model and true parameters. Values of 1 indicate optimal performance. Columns indicate different models, rows different noise levels, and colors indicate the parameter set used. Grouped over stochastic replicates applying the contol policy and stochastic replicates of training data generated from the model indicated, see raw data for details. Randomly chosen parameter values for the models shown in tables below."}
ggplot(sensitivity_dat) + 
  geom_histogram(aes(value, fill=parameters)) + 
  xlim(0,1.0) + 
  theme_bw() + 
  xlab("value as fraction of the optimal") + 
  facet_grid(noise~model)
write.csv(sensitivity_dat, "components/data/sensitivity_dat.csv") 
```

```{r}
pandoc.table(AllenParameterSets, caption="Randomly chosen parameter sets for the Allen models in Figure S9." )
```
```{r}
pandoc.table(AllenParameterSets, caption="Randomly chosen parameter sets for the Myers models in Figure S9." )
```

```{r include=FALSE}
unlink("ricker_process.bugs")
unlink("allen_process.bugs")
unlink("myers_process.bugs")
```
